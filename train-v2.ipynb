{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32\n",
    "data_dir = \"images_new\"\n",
    "nsfw_classes_data = [{\"name\": \"general_nsfw\",\"index\": 5}]\n",
    "binary_classes_names = [\"adult\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/male_underware/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "found 435 for class male_shirtless\n",
      "found 2107 for class general_not_nsfw_not_suggestive\n",
      "found 153 for class male_underware\n",
      "found 798 for class female_nudity\n",
      "found 927 for class female_swimwear\n",
      "found 933 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/male_underware/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 4279 images belonging to 6 classes.\n",
      "Found 1066 images belonging to 6 classes.\n",
      "class_weights =>  {0: 0.9187371567345414, 1: 0.6063889407808705, 2: 0.9714178965066318, 3: 0.8509247151130207, 4: 0.8268260788342985, 5: 0.825705212030637}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_extractor_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-361-f2d24f9e73f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model = tf.keras.Sequential([\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfeature_extractor_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hidden_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_extractor_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(num_classes, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "    layers.Dense(N_LABELS, activation='sigmoid', name='output')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which we predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5 # Keep it small when transfer learning\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#   optimizer=\"adam\",\n",
    "#   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#   # metrics=[\"accuracy\", f1_metric])\n",
    "#   metrics=[tfa.metrics.FBetaScore(num_classes=num_classes,average=\"micro\",threshold=0.9)]\n",
    "# )\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=macro_soft_f1,\n",
    "  metrics=[macro_f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "134/134 [==============================] - 244s 2s/step - loss: 0.6059 - macro_f1: 0.2190 - val_loss: 0.7028 - val_macro_f1: 0.2155\n",
      "Epoch 2/30\n",
      "134/134 [==============================] - 251s 2s/step - loss: 0.6026 - macro_f1: 0.2232 - val_loss: 0.6984 - val_macro_f1: 0.2227\n",
      "Epoch 3/30\n",
      "134/134 [==============================] - 297s 2s/step - loss: 0.6007 - macro_f1: 0.2239 - val_loss: 0.6963 - val_macro_f1: 0.2252\n",
      "Epoch 4/30\n",
      " 11/134 [=>............................] - ETA: 3:44 - loss: 0.6005 - macro_f1: 0.2282"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch = num_classes//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy', #'val_prc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=3,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=EPOCHS,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/.DS_Store/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800a38fee74a414980b57e151d9decb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6affca327a443069d8010fc90e98106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeecf2d58e1423291ef83a225414bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/1642616763.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{}.h5\".format(int(t))\n",
    "#export_path_keras = \"models/{}.h5\".format(\"current-model-best-weight\")\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path_keras = \"models/1625175782.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/male_underware/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21514ef469584e588b0693d11652f6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79e6956537148679e2c33af637d749a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f70998953aa41a2aedff2b8564a8307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  3000\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def predict_single_image_from_path(path):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(numpy.array([image_resized]))\n",
    "    class_index = 0\n",
    "    max = 0\n",
    "    for i in range(0, len(prediction[0])):\n",
    "        if prediction[0][0] > max:\n",
    "            max =  prediction[0][i]\n",
    "            class_index = i\n",
    "    print(prediction[0])\n",
    "    # class_index = int(np.argmax(prediction[0], axis=0)) #numpy.where(prediction[0]== numpy.amax(prediction[0]))[0][0]\n",
    "    return class_index, class_names[class_index], Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        class_index, class_name, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(class_name)\n",
    "        display(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        class_index, class_name, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(class_name)\n",
    "        display(image)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [5 3 3 1 2 1 2 3 3 3 2 2 2 3 1 3 2 5 1 3 0 2 2 2 2 5 1 2 1 2 2 1]\n",
      "Predicted labels:  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n",
      "precisions :  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6213079"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_prediction(predictions):\n",
    "    binary_classes_index = []\n",
    "    predictions_probs = []\n",
    "    predictions_data = [] \n",
    "    numpy_predictions = predictions.numpy()\n",
    "    binary_class_names = []\n",
    "    for prediction in numpy_predictions:\n",
    "        nsfw_pred_sum = 0\n",
    "        binary_class_index = 0\n",
    "        for nsfw_classe_data in nsfw_classes_data:\n",
    "            nsfw_pred_sum += prediction[nsfw_classe_data[\"index\"]]\n",
    "\n",
    "        nsfw_pred_prob = nsfw_pred_sum / len(nsfw_classes_data)\n",
    "        \n",
    "        binary_class_index = 0 if nsfw_pred_prob > 0.5 else 1\n",
    "        binary_classes_index.append(nsfw_pred_prob)\n",
    "        predictions_probs.append(nsfw_pred_prob)\n",
    "\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "        binary_class_names.append(binary_classes_names[binary_class_index])\n",
    "    return np.array(binary_classes_index), np.array(predictions_probs), predictions, binary_class_names, predictions_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_ids, num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)\n",
    "0.6213079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-8d65b8d6993c>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-ec2fc7a40586>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnumpy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbinary_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('holypics-SxDLhKSZ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
