{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {zZZ\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "    <li>Work  with imbalanced dataset => https://medium.com/geekculture/imbalanced-dataset-machine-learning-model-from-end-to-end-implementation-tensorflow-2-2-c48b5bc2eabc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.5.5.62-cp36-abi3-win_amd64.whl (35.3 MB)\n",
      "Collecting tensorflow_hub\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow_gpu-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements-notebook.txt (line 4)) (1.0.2)\n",
      "Requirement already satisfied: tensorflow_addons in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements-notebook.txt (line 5)) (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opencv-python-headless->-r requirements-notebook.txt (line 1)) (1.22.1)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     ------------------------------------- 895.7/895.7 KB 75.5 kB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 54.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.16.0)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     ------------------------------------- 126.7/126.7 KB 81.0 kB/s eta 0:00:00\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "     ------------------------------------- 462.5/462.5 KB 72.5 kB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-win_amd64.whl (2.8 MB)\n",
      "     ---------------------------------------- 2.8/2.8 MB 62.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (4.0.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     --------------------------------------- 57.5/57.5 KB 60.5 kB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     --------------------------------------- 65.5/65.5 KB 54.4 kB/s eta 0:00:00\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "     --------------------------------       11.9/13.9 MB 124.4 kB/s eta 0:00:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 165, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 339, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 373, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 204, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 215, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 288, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 299, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 487, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 532, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 214, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 94, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 146, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 304, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements-notebook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T17:38:43.639206Z",
     "iopub.status.busy": "2022-01-27T17:38:43.638895Z",
     "iopub.status.idle": "2022-01-27T17:38:46.422808Z",
     "shell.execute_reply": "2022-01-27T17:38:46.422237Z",
     "shell.execute_reply.started": "2022-01-27T17:38:43.639180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.5.0\n",
      "Uninstalling tensorflow-2.5.0:\n",
      "  Successfully uninstalled tensorflow-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://storage.googleapis.com/kaggle-data-sets/1885605/3083816/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220127%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220127T102630Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=173a91a8aaced400cb01f48648c997f0aac12cdbd597c4a48ae4089925004936e9e3688a71f69507c76736a13a5553f862af96fff8de4c9bc9ffea29d1ae8c2e976924e3eb329d6fd7f176b8dfb1b33cfb9fa3b000e3e916d15e55a72d9b8d8be048a808fb8fad044e59a79ad88a80fce3d8fb2fe98665dbe6be336ba8dd182f0a2b14bfa8efe46e201d94d9947d30d2f48f82925bbcaab47ab6a0b88dc7617a94704b0c84999388b312644910e369805627e3f95ce6c0f2103da2252947f46f954e20c0360c47763139ad27530fccc3d87bed953ef53005237c8a262a733446958bcf9d79196bd131c8bbfb07a8c67ce1062082221137bc336c25c0f4431fa7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T10:28:01.595074Z",
     "iopub.status.busy": "2022-01-27T10:28:01.594852Z",
     "iopub.status.idle": "2022-01-27T10:28:02.334565Z",
     "shell.execute_reply": "2022-01-27T10:28:02.333899Z",
     "shell.execute_reply.started": "2022-01-27T10:28:01.595052Z"
    }
   },
   "outputs": [],
   "source": [
    "!mv archive.zip* archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "#from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No compatible GPU device found\n"
     ]
    }
   ],
   "source": [
    "phisical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print(phisical_devices)\n",
    "if len(phisical_devices) > 0: \n",
    "    tf.config.experimental.set_memory_growth(phisical_devices[0], True)\n",
    "    print(\"GPU activated with {}\".format(phisical_devices[0]))\n",
    "else:\n",
    "    print(\"No compatible GPU device found\")\n",
    "# print(tf.test.is_gpu_available())\n",
    "# print(tf.config.list_pZZzhysical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "EPOCHS=30\n",
    "PATIENCE=3\n",
    "LR = 1e-4\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32#32\n",
    "data_dir = \"images_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "def interpret_prediction(predicted_batch, get_images=False, image_set=[]):\n",
    "    # np_prediction = predicted_batch.numpy()\n",
    "    decoded_predictions = []\n",
    "    decoded_main_predictions_classes = []\n",
    "    max_indices = [(lambda pr: class_names[np.argmax(pr, axis=-1)])(predicton) for predicton in predicted_batch]\n",
    "    for count in range(0, len(predicted_batch)):\n",
    "        prd_btch = predicted_batch[count]\n",
    "        decoded_part = []\n",
    "        for i in range(0, num_classes):\n",
    "            decoded_prediction = {}\n",
    "            decoded_prediction[\"class_name\"] = class_names[i]\n",
    "            try:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i].numpy()\n",
    "            except Exception as e:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i]\n",
    "            decoded_prediction[\"precision\"] = np.sum(prd_btch[i]) / num_classes\n",
    "            \n",
    "            # decoded_prediction[\"count_index\"] = count\n",
    "        \n",
    "            if get_images:\n",
    "                decoded_prediction[\"image\"] = image_set[count]\n",
    "            decoded_part.append(decoded_prediction)\n",
    "        decoded_predictions.append(decoded_part)\n",
    "        \n",
    "        decoded_main_predictions_classes.append(decoded_part)\n",
    "    return decoded_predictions, decoded_main_predictions_classes, max_indices\n",
    "    \n",
    "\n",
    "def predict_single_image_from_path(path, break_line=True):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def predict_single_raw_image(image, break_line=True):\n",
    "    prediction = model.predict(image)\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    \n",
    "    return to_print, image\n",
    "\n",
    "\n",
    "def predict_single_image_from_url(url, break_line=True):\n",
    "    image = imutils.url_to_image(url)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "\n",
    "def predict_from_path(path=data_dir, group=True):\n",
    "    data_dir = path\n",
    "    clean_up_data_dir()\n",
    "    images_path = []\n",
    "    \n",
    "    if(group):\n",
    "        data_sub_directories = os.listdir(data_dir)\n",
    "        for data_sub_directory in data_sub_directories:\n",
    "            # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "            print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))\n",
    "            for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "                images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "    else:\n",
    "        try:\n",
    "            for current_dir in os.listdir(data_dir):\n",
    "                images_path.append(os.path.join(data_dir, current_dir))\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "\n",
    "    if not group:\n",
    "        data_dir = \".\"\n",
    "    \n",
    "    bulk_prediction(data_dir, images_path)\n",
    "    \n",
    "def bulk_prediction(data_dir=\"\", images_path=[], images=[]):\n",
    "    current = 0\n",
    "    output = widgets.Output()\n",
    "    next_button = widgets.Button(description='Next')\n",
    "    prev_button = widgets.Button(description='Prev')\n",
    "    display_current_button = widgets.Button(description='Current')\n",
    "    current_index_text = widgets.Textarea(\n",
    "        value=str(current),\n",
    "        placeholder='current index goes here',\n",
    "        description='index',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    display(current_index_text, display_current_button, prev_button, next_button, output)\n",
    "    \n",
    "    def default_action():\n",
    "        global current\n",
    "        with output:\n",
    "            clear_output()\n",
    "            images_store = images_path if len(images_path) > 0 else images\n",
    "            \n",
    "            print(\"{0}/{1}\".format(current+1, len(images_store)))\n",
    "            if len(images_path) > 0:\n",
    "                to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "            else:\n",
    "                to_print, image = predict_single_raw_image(images[current])\n",
    "            print(to_print)\n",
    "            display(image)\n",
    "            \n",
    "    def on_next_button_clicked(_):\n",
    "        global current\n",
    "        if current+2 > len(images_path):\n",
    "            return None\n",
    "        current+=1\n",
    "        default_action()\n",
    "\n",
    "\n",
    "    def on_prev_button_clicked(_):\n",
    "        global current\n",
    "        if current-1 < 0:\n",
    "            return None\n",
    "        current-=1\n",
    "        default_action()\n",
    "        \n",
    "        \n",
    "    def on_current_index_change(_):\n",
    "        update_index_change(current_index_text.value)\n",
    "\n",
    "    def update_index_change(indexString):\n",
    "        global current\n",
    "        try:\n",
    "            current = int(indexString)\n",
    "            default_action()\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "\n",
    "    next_button.on_click(on_next_button_clicked)\n",
    "    prev_button.on_click(on_prev_button_clicked)\n",
    "    display_current_button.on_click(on_current_index_change)\n",
    "    current_index_text.on_displayed(update_index_change(str(current)))\n",
    "    \n",
    "\n",
    "def predict_at_random():    \n",
    "    base_url = \"https://picsum.photos/224/224\"\n",
    "    again_button = widgets.Button(description='Again')\n",
    "    output = widgets.Output()\n",
    "    display(again_button, output)\n",
    "\n",
    "    def on_again_button_clicked(_):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            to_print, image = predict_single_image_from_url(base_url)\n",
    "            print(to_print)\n",
    "            display(image)\n",
    "\n",
    "    again_button.on_click(on_again_button_clicked)\n",
    "    \n",
    "    \n",
    "def predict_url_batch(urls, figsize=(30, 30), verbose=False, break_line=True):\n",
    "    predictions_output = []    \n",
    "    images=[]\n",
    "    for url in urls:\n",
    "        try:\n",
    "            image = imutils.url_to_image(url)\n",
    "            # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "            imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "            image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "            images.append(np.array([image_resized]))\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "    bulk_prediction(images=images)\n",
    "    \n",
    "def predict_from_txt_file(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False, break_line=True):\n",
    "    urls = []\n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)       \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predict_url_batch(urls, figsize=figsize, verbose=verbose, break_line=break_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9827860c6d4e6a9462c35d225446ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='0', description='index', placeholder='current index goes here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a35fa192014d63af096066c0f7d448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Current', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f96a2c6b242e88c4aeaa84571d704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8fafdfb21c40af91e5fa1c676f1dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c475f4232b2345ffbf6895746e2dcd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_urls = [\n",
    "    \"https://i.ytimg.com/vi/yWI61kpFEAA/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDRAPwFXV09U5Eo-fhoUnh7FTbp1w\",\n",
    "    \"https://i.ytimg.com/vi/EiXQmeuHTOY/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBz-YYzwt-B30cjMrXYzm0PopCukg\",\n",
    "    \"https://i.ytimg.com/vi/poQXNp9ItL4/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLAyT3wtstrlzKYaC9sGf05ea66wmg\"\n",
    "]\n",
    "predict_url_batch(current_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'images_newfemale_swimwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_not_nsfw_not_suggestive.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_nsfw.*': No such file or directory\n",
      "rm: cannot remove 'images_newpeople_nudity.*': No such file or directory\n",
      "rm: cannot remove 'images_newpeople_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_new/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'images_new/.DS_Store': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 739 for class female_swimwear\n",
      "found 1668 for class general_not_nsfw_not_suggestive\n",
      "found 1500 for class general_nsfw\n",
      "found 1500 for class people_nudity\n",
      "found 1668 for class people_underwear\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'images_newfemale_swimwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_not_nsfw_not_suggestive.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_nsfw.*': No such file or directory\n",
      "rm: cannot remove 'images_newpeople_nudity.*': No such file or directory\n",
      "rm: cannot remove 'images_newpeople_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_new/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'images_new/.DS_Store': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5655 images belonging to 5 classes.\n",
      "Found 1412 images belonging to 5 classes.\n",
      "class_weights =>  {0: 0.895547703180212, 1: 0.7642402826855124, 2: 0.7879858657243817, 3: 0.7879858657243817, 4: 0.7642402826855124}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "num_samples = training_set.samples + validation_set.samples\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_4 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    # layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "    # tf.keras.layers.Flatten(),\n",
    "    layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "])\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(num_classes, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=macro_soft_f1,#\"categorical_crossentropy\",\n",
    "  metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# model.compile(\n",
    "#   optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "#   loss=macro_soft_f1,\n",
    "#   metrics=[macro_f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "177/177 [==============================] - 214s 1s/step - loss: -0.2849 - accuracy: 0.1052 - val_loss: -0.3633 - val_accuracy: 0.1211\n",
      "Epoch 2/30\n",
      "177/177 [==============================] - 207s 1s/step - loss: -0.3674 - accuracy: 0.1073 - val_loss: -0.3731 - val_accuracy: 0.1232\n",
      "Epoch 3/30\n",
      "177/177 [==============================] - 206s 1s/step - loss: -0.3739 - accuracy: 0.1089 - val_loss: -0.3749 - val_accuracy: 0.1246\n",
      "Epoch 4/30\n",
      "177/177 [==============================] - 205s 1s/step - loss: -0.3761 - accuracy: 0.1111 - val_loss: -0.3778 - val_accuracy: 0.1239\n",
      "Epoch 5/30\n",
      "177/177 [==============================] - 206s 1s/step - loss: -0.3769 - accuracy: 0.1116 - val_loss: -0.3768 - val_accuracy: 0.1225\n",
      "Epoch 6/30\n",
      "177/177 [==============================] - 205s 1s/step - loss: -0.3780 - accuracy: 0.1109 - val_loss: -0.3786 - val_accuracy: 0.1225\n",
      "Epoch 7/30\n",
      "177/177 [==============================] - 204s 1s/step - loss: -0.3775 - accuracy: 0.1102 - val_loss: -0.3790 - val_accuracy: 0.1225\n",
      "Epoch 8/30\n",
      "177/177 [==============================] - 204s 1s/step - loss: -0.3785 - accuracy: 0.1096 - val_loss: -0.3788 - val_accuracy: 0.1225\n",
      "Epoch 9/30\n",
      "177/177 [==============================] - 206s 1s/step - loss: -0.3782 - accuracy: 0.1102 - val_loss: -0.3790 - val_accuracy: 0.1211\n",
      "Epoch 10/30\n",
      "177/177 [==============================] - 206s 1s/step - loss: -0.3781 - accuracy: 0.1100 - val_loss: -0.3792 - val_accuracy: 0.1204\n",
      "Epoch 11/30\n",
      "177/177 [==============================] - 206s 1s/step - loss: -0.3787 - accuracy: 0.1098 - val_loss: -0.3789 - val_accuracy: 0.1197\n",
      "Epoch 12/30\n",
      "177/177 [==============================] - 205s 1s/step - loss: -0.3791 - accuracy: 0.1093 - val_loss: -0.3793 - val_accuracy: 0.1197\n",
      "Epoch 13/30\n",
      "177/177 [==============================] - 206s 1s/step - loss: -0.3785 - accuracy: 0.1073 - val_loss: -0.3792 - val_accuracy: 0.1190\n",
      "Epoch 14/30\n",
      "177/177 [==============================] - 207s 1s/step - loss: -0.3794 - accuracy: 0.1056 - val_loss: -0.3789 - val_accuracy: 0.1197\n",
      "Epoch 15/30\n",
      "177/177 [==============================] - 208s 1s/step - loss: -0.3785 - accuracy: 0.1054 - val_loss: -0.3795 - val_accuracy: 0.1197\n",
      "Epoch 16/30\n",
      "177/177 [==============================] - 215s 1s/step - loss: -0.3784 - accuracy: 0.1045 - val_loss: -0.3800 - val_accuracy: 0.1176\n",
      "Epoch 17/30\n",
      "177/177 [==============================] - 206s 1s/step - loss: -0.3789 - accuracy: 0.1057 - val_loss: -0.3807 - val_accuracy: 0.1169\n",
      "Epoch 18/30\n",
      "177/177 [==============================] - 204s 1s/step - loss: -0.3791 - accuracy: 0.1047 - val_loss: -0.3801 - val_accuracy: 0.1154\n",
      "Epoch 19/30\n",
      "177/177 [==============================] - 204s 1s/step - loss: -0.3792 - accuracy: 0.1072 - val_loss: -0.3791 - val_accuracy: 0.1133\n",
      "Epoch 20/30\n",
      "177/177 [==============================] - 205s 1s/step - loss: -0.3790 - accuracy: 0.1068 - val_loss: -0.3799 - val_accuracy: 0.1133\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_samples//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=PATIENCE,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=EPOCHS,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # callbacks=[model_checkpoint_callback],\n",
    "#                     class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'categorical_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [95]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# acc = history.history['accuracy']\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# val_acc = history.history['accuracy']\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'categorical_accuracy'"
     ]
    }
   ],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mobilenet_v2_1643880288.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 10248     \n",
      "=================================================================\n",
      "Total params: 2,268,232\n",
      "Trainable params: 10,248\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "export_path_keras = \"models/mobilenet_v2_1643808443.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Ids:            [2 3 2 2 3 4 0 1 4 1 1 4 2 3 4 1 4 3 2 2 1 3 0 3 1 2 0 4 1 3 3 4]\n",
      "predicted_class_names:            ['gw', 'py', 'gw', 'gw', 'py', 'pr', 'fr', 'ge', 'pr', 'ge', 'ge', 'pr', 'gw', 'py', 'pr', 'ge', 'pr', 'py', 'gw', 'gw', 'ge', 'py', 'fr', 'py', 'ge', 'gw', 'fr', 'pr', 'ge', 'py', 'py', 'pr']\n",
      "three_digit_predictions:  [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99]\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "# tf_decoded_predictions = tf.keras.applications.imagenet_utils.decode_predictions(model.predict(image_batch))\n",
    "\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_class_names = [(lambda l, cl: cl[l][0]+cl[l][len(cl[l])-1])(label, class_names) for label in label_batch]\n",
    "three_digit_predictions = [(lambda prb: prb*100 if str(prb*100).replace(\",\", \".\").find(\".\") == -1 else int(str(prb*100).split(\".\")[0].replace(\"[\", \"\"))/100 )(prb) for prb in predicted_batch.numpy()]\n",
    "print(\"Labels Ids:           \", label_batch)\n",
    "print(\"predicted_class_names:           \",   predicted_class_names)\n",
    "print(\"three_digit_predictions: \", three_digit_predictions)\n",
    "# print(  (lambda x: x[x.index(max(x))]  )(three_digit_predictions) )\n",
    "print( three_digit_predictions[np.argmax(three_digit_predictions)] )\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'images_newfemale_swimwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_not_nsfw_not_suggestive.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_nsfw.*': No such file or directory\n",
      "rm: cannot remove 'images_newpeople_nudity.*': No such file or directory\n",
      "rm: cannot remove 'images_newpeople_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_new/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'images_new/.DS_Store': No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c2a25822ce48cda409560317247b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='0', description='index', placeholder='current index goes here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a529122ce63a4989bfce39a71cc4cddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Current', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a7b9a3f2554a399aa2ddc8501e8c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c333f821ff42ad947cb726eb45a9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b8d9ad55894ccebbbdf25bedd97202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_path(\"images_backup/test/neutral\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490570594deb4d24a3f97e1ff1bcc7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Again', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef946fc05b04677905383de011660be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_at_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e1901e493748e1b0663d378b13fe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='0', description='index', placeholder='current index goes here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a805a6fed0414fb4bcc737286a6eae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Current', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff5c3621374c148f290ce4ea298045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e809bc05ae645c79eeb7954bec15ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c21123224c9403a961b757d39832a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_urls = [\n",
    "    \"https://i.ytimg.com/vi/yWI61kpFEAA/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDRAPwFXV09U5Eo-fhoUnh7FTbp1w\",\n",
    "    \"https://i.ytimg.com/vi/EiXQmeuHTOY/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBz-YYzwt-B30cjMrXYzm0PopCukg\",\n",
    "    \"https://i.ytimg.com/vi/poQXNp9ItL4/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLAyT3wtstrlzKYaC9sGf05ea66wmg\"\n",
    "]\n",
    "predict_url_batch(current_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_from_txt_file(src='validation-adult-save.txt', start=30, limit=40, break_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
