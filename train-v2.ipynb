{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {zZZ\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "    <li>Work  with imbalanced dataset => https://medium.com/geekculture/imbalanced-dataset-machine-learning-model-from-end-to-end-implementation-tensorflow-2-2-c48b5bc2eabc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.5.5.62-cp36-abi3-win_amd64.whl (35.3 MB)\n",
      "Collecting tensorflow_hub\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow_gpu-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements-notebook.txt (line 4)) (1.0.2)\n",
      "Requirement already satisfied: tensorflow_addons in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements-notebook.txt (line 5)) (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opencv-python-headless->-r requirements-notebook.txt (line 1)) (1.22.1)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     ------------------------------------- 895.7/895.7 KB 75.5 kB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 54.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.16.0)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     ------------------------------------- 126.7/126.7 KB 81.0 kB/s eta 0:00:00\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "     ------------------------------------- 462.5/462.5 KB 72.5 kB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-win_amd64.whl (2.8 MB)\n",
      "     ---------------------------------------- 2.8/2.8 MB 62.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\israel\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (4.0.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     --------------------------------------- 57.5/57.5 KB 60.5 kB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     --------------------------------------- 65.5/65.5 KB 54.4 kB/s eta 0:00:00\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "     --------------------------------       11.9/13.9 MB 124.4 kB/s eta 0:00:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 165, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 339, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 373, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 204, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 215, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 288, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 299, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 487, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 532, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 214, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 94, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 146, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 304, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements-notebook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T17:38:43.639206Z",
     "iopub.status.busy": "2022-01-27T17:38:43.638895Z",
     "iopub.status.idle": "2022-01-27T17:38:46.422808Z",
     "shell.execute_reply": "2022-01-27T17:38:46.422237Z",
     "shell.execute_reply.started": "2022-01-27T17:38:43.639180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.5.0\n",
      "Uninstalling tensorflow-2.5.0:\n",
      "  Successfully uninstalled tensorflow-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://storage.googleapis.com/kaggle-data-sets/1885605/3083816/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220127%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220127T102630Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=173a91a8aaced400cb01f48648c997f0aac12cdbd597c4a48ae4089925004936e9e3688a71f69507c76736a13a5553f862af96fff8de4c9bc9ffea29d1ae8c2e976924e3eb329d6fd7f176b8dfb1b33cfb9fa3b000e3e916d15e55a72d9b8d8be048a808fb8fad044e59a79ad88a80fce3d8fb2fe98665dbe6be336ba8dd182f0a2b14bfa8efe46e201d94d9947d30d2f48f82925bbcaab47ab6a0b88dc7617a94704b0c84999388b312644910e369805627e3f95ce6c0f2103da2252947f46f954e20c0360c47763139ad27530fccc3d87bed953ef53005237c8a262a733446958bcf9d79196bd131c8bbfb07a8c67ce1062082221137bc336c25c0f4431fa7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T10:28:01.595074Z",
     "iopub.status.busy": "2022-01-27T10:28:01.594852Z",
     "iopub.status.idle": "2022-01-27T10:28:02.334565Z",
     "shell.execute_reply": "2022-01-27T10:28:02.333899Z",
     "shell.execute_reply.started": "2022-01-27T10:28:01.595052Z"
    }
   },
   "outputs": [],
   "source": [
    "!mv archive.zip* archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "#from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import imutils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No compatible GPU device found\n"
     ]
    }
   ],
   "source": [
    "phisical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print(phisical_devices)\n",
    "if len(phisical_devices) > 0: \n",
    "    tf.config.experimental.set_memory_growth(phisical_devices[0], True)\n",
    "    print(\"GPU activated with {}\".format(phisical_devices[0]))\n",
    "else:\n",
    "    print(\"No compatible GPU device found\")\n",
    "# print(tf.test.is_gpu_available())\n",
    "# print(tf.config.list_pZZzhysical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "EPOCHS=30\n",
    "PATIENCE=3\n",
    "LR = 1e-4\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32#32\n",
    "data_dir = \"images_new\"\n",
    "csv_dataset = \"image_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "def interpret_prediction(predicted_batch, get_images=False, image_set=[]):\n",
    "    # np_prediction = predicted_batch.numpy()\n",
    "    decoded_predictions = []\n",
    "    decoded_main_predictions_classes = []\n",
    "    max_indices = [(lambda pr: class_names[np.argmax(pr, axis=-1)])(predicton) for predicton in predicted_batch]\n",
    "    for count in range(0, len(predicted_batch)):\n",
    "        prd_btch = predicted_batch[count]\n",
    "        decoded_part = []\n",
    "        for i in range(0, num_classes):\n",
    "            decoded_prediction = {}\n",
    "            decoded_prediction[\"class_name\"] = class_names[i]\n",
    "            try:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i].numpy()\n",
    "            except Exception as e:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i]\n",
    "            decoded_prediction[\"precision\"] = np.sum(prd_btch[i]) / num_classes\n",
    "            \n",
    "            # decoded_prediction[\"count_index\"] = count\n",
    "        \n",
    "            if get_images:\n",
    "                decoded_prediction[\"image\"] = image_set[count]\n",
    "            decoded_part.append(decoded_prediction)\n",
    "        decoded_predictions.append(decoded_part)\n",
    "        \n",
    "        decoded_main_predictions_classes.append(decoded_part)\n",
    "    return decoded_predictions, decoded_main_predictions_classes, max_indices\n",
    "    \n",
    "\n",
    "def predict_single_image_from_path(path, break_line=True):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def predict_single_raw_image(image, break_line=True):\n",
    "    prediction = model.predict(image)\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    \n",
    "    return to_print, image\n",
    "\n",
    "\n",
    "def predict_single_image_from_url(url, break_line=True):\n",
    "    image = imutils.url_to_image(url)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "\n",
    "def predict_from_path(path=data_dir, group=True):\n",
    "    data_dir = path\n",
    "    clean_up_data_dir()\n",
    "    images_path = []\n",
    "    \n",
    "    if(group):\n",
    "        data_sub_directories = os.listdir(data_dir)\n",
    "        for data_sub_directory in data_sub_directories:\n",
    "            # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "            print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))\n",
    "            for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "                images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "    else:\n",
    "        try:\n",
    "            for current_dir in os.listdir(data_dir):\n",
    "                images_path.append(os.path.join(data_dir, current_dir))\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "\n",
    "    if not group:\n",
    "        data_dir = \".\"\n",
    "    \n",
    "    bulk_prediction(data_dir, images_path)\n",
    "    \n",
    "def bulk_prediction(data_dir=\"\", images_path=[], images=[]):\n",
    "    current = 0\n",
    "    output = widgets.Output()\n",
    "    next_button = widgets.Button(description='Next')\n",
    "    prev_button = widgets.Button(description='Prev')\n",
    "    display_current_button = widgets.Button(description='Current')\n",
    "    current_index_text = widgets.Textarea(\n",
    "        value=str(current),\n",
    "        placeholder='current index goes here',\n",
    "        description='index',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    display(current_index_text, display_current_button, prev_button, next_button, output)\n",
    "    \n",
    "    def default_action():\n",
    "        global current\n",
    "        with output:\n",
    "            clear_output()\n",
    "            images_store = images_path if len(images_path) > 0 else images\n",
    "            \n",
    "            print(\"{0}/{1}\".format(current+1, len(images_store)))\n",
    "            if len(images_path) > 0:\n",
    "                to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "            else:\n",
    "                to_print, image = predict_single_raw_image(images[current])\n",
    "            print(to_print)\n",
    "            display(image)\n",
    "            \n",
    "    def on_next_button_clicked(_):\n",
    "        global current\n",
    "        if current+2 > len(images_path):\n",
    "            return None\n",
    "        current+=1\n",
    "        default_action()\n",
    "\n",
    "\n",
    "    def on_prev_button_clicked(_):\n",
    "        global current\n",
    "        if current-1 < 0:\n",
    "            return None\n",
    "        current-=1\n",
    "        default_action()\n",
    "        \n",
    "        \n",
    "    def on_current_index_change(_):\n",
    "        update_index_change(current_index_text.value)\n",
    "\n",
    "    def update_index_change(indexString):\n",
    "        global current\n",
    "        try:\n",
    "            current = int(indexString)\n",
    "            default_action()\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "\n",
    "    next_button.on_click(on_next_button_clicked)\n",
    "    prev_button.on_click(on_prev_button_clicked)\n",
    "    display_current_button.on_click(on_current_index_change)\n",
    "    current_index_text.on_displayed(update_index_change(str(current)))\n",
    "    \n",
    "\n",
    "def predict_at_random():    \n",
    "    base_url = \"https://picsum.photos/224/224\"\n",
    "    again_button = widgets.Button(description='Again')\n",
    "    output = widgets.Output()\n",
    "    display(again_button, output)\n",
    "\n",
    "    def on_again_button_clicked(_):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            to_print, image = predict_single_image_from_url(base_url)\n",
    "            print(to_print)\n",
    "            display(image)\n",
    "\n",
    "    again_button.on_click(on_again_button_clicked)\n",
    "    \n",
    "    \n",
    "def predict_url_batch(urls, figsize=(30, 30), verbose=False, break_line=True):\n",
    "    predictions_output = []    \n",
    "    images=[]\n",
    "    for url in urls:\n",
    "        try:\n",
    "            image = imutils.url_to_image(url)\n",
    "            # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "            imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "            image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "            images.append(np.array([image_resized]))\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "    bulk_prediction(images=images)\n",
    "    \n",
    "def predict_from_txt_file(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False, break_line=True):\n",
    "    urls = []\n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)       \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predict_url_batch(urls, figsize=figsize, verbose=verbose, break_line=break_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "found 429 for class male_shirtless\n",
      "found 2767 for class general_not_nsfw_not_suggestive\n",
      "found 1512 for class female_underwear\n",
      "found 1500 for class female_nudity\n",
      "found 156 for class male_underwear\n",
      "found 739 for class female_swimwear\n",
      "found 1500 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 6876 validated image filenames.\n",
      "Found 1719 validated image filenames.\n",
      "class_weights =>  {0: 0.9501336742996629, 1: 0.6783680111588981, 2: 0.8242473555736372, 3: 0.8256422178309892, 4: 0.9818667906544228, 5: 0.9140997326514007, 6: 0.8256422178309892}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 8 invalid image filename(s) in x_col=\"filenames\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(csv_dataset)\n",
    "columns=data_sub_directories\n",
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "training_set=train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=data_dir,\n",
    "    x_col=\"filenames\",\n",
    "    y_col=columns,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode=\"raw\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "validation_set=train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=data_dir,\n",
    "    x_col=\"filenames\",\n",
    "    y_col=columns,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode=\"raw\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "class_names = columns\n",
    "num_classes = len(class_names)\n",
    "num_samples = training_set.samples + validation_set.samples\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean_up_data_dir()\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "#     #rotation_range=10,\n",
    "#     #brightness_range=[0.2,1.2],\n",
    "#     #shear_range=0.2,\n",
    "#     #zoom_range=0.4,\n",
    "#     #horizontal_flip=True,\n",
    "#     validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training_set = train_datagen.flow_from_directory(\n",
    "#     data_dir,\n",
    "#     target_size=dimensions,\n",
    "#     batch_size=batch_size,\n",
    "#     # class_mode='categorical',\n",
    "#     class_mode='sparse',\n",
    "#     subset='training') # set as training data\n",
    "\n",
    "# validation_set = train_datagen.flow_from_directory(\n",
    "#     data_dir, # same directory as training data\n",
    "#     target_size=dimensions,\n",
    "#     batch_size=batch_size,\n",
    "#     # class_mode='categorical',\n",
    "#     class_mode='sparse',\n",
    "#     subset='validation') # set as validation data\n",
    "\n",
    "# class_names = list(training_set.class_indices)\n",
    "# num_classes = len(class_names)\n",
    "# num_samples = training_set.samples + validation_set.samples\n",
    "# files_per_class = []\n",
    "# for folder in os.listdir(data_dir):\n",
    "#     if not os.path.isfile(folder):\n",
    "#             files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "# total_files = sum(files_per_class)\n",
    "# class_weights = {}\n",
    "# for i in range(len(files_per_class)):\n",
    "#     class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "# print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 8967      \n",
      "=================================================================\n",
      "Total params: 2,266,951\n",
      "Trainable params: 8,967\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     # layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     # tf.keras.layers.Flatten(),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(num_classes, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "#     tf.keras.optimizers.RMSprop(\n",
    "#     learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n",
    "#     name='RMSprop', **kwargs\n",
    "#     )\n",
    "  optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),\n",
    "  loss=\"binary_crossentropy\",\n",
    "  metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# model.compile\n",
    "#   optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "#   loss=macro_soft_f1,#\"categorical_crossentropy\",\n",
    "#   metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#   optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "#   loss=macro_soft_f1,\n",
    "#   metrics=[macro_f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "215/215 [==============================] - 144s 657ms/step - loss: 0.4654 - accuracy: 0.3569 - val_loss: 0.4117 - val_accuracy: 0.4218\n",
      "Epoch 2/30\n",
      "215/215 [==============================] - 166s 771ms/step - loss: 0.3187 - accuracy: 0.4471 - val_loss: 0.3394 - val_accuracy: 0.4136\n",
      "Epoch 3/30\n",
      "215/215 [==============================] - 183s 852ms/step - loss: 0.2723 - accuracy: 0.4554 - val_loss: 0.3279 - val_accuracy: 0.3903\n",
      "Epoch 4/30\n",
      "215/215 [==============================] - 211s 982ms/step - loss: 0.2510 - accuracy: 0.4519 - val_loss: 0.3253 - val_accuracy: 0.3700\n",
      "Epoch 5/30\n",
      "215/215 [==============================] - 226s 1s/step - loss: 0.2382 - accuracy: 0.4549 - val_loss: 0.3349 - val_accuracy: 0.3223\n",
      "Epoch 6/30\n",
      "215/215 [==============================] - 842s 4s/step - loss: 0.2293 - accuracy: 0.4549 - val_loss: 0.3461 - val_accuracy: 0.3490\n",
      "Epoch 7/30\n",
      "215/215 [==============================] - 191s 891ms/step - loss: 0.2228 - accuracy: 0.4555 - val_loss: 0.3494 - val_accuracy: 0.3392\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_samples//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=PATIENCE,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=EPOCHS,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # callbacks=[model_checkpoint_callback],\n",
    "#                     class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['accuracy']\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mobilenet_v2_1643893593.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 10248     \n",
      "=================================================================\n",
      "Total params: 2,268,232\n",
      "Trainable params: 10,248\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "export_path_keras = \"models/mobilenet_v2_1643808443.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f3d5481aa600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredicted_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredicted_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mthree_digit_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mprb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels Ids:           \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f3d5481aa600>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredicted_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredicted_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mthree_digit_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mprb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels Ids:           \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f3d5481aa600>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(l, cl)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredicted_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredicted_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mthree_digit_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mprb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels Ids:           \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "# tf_decoded_predictions = tf.keras.applications.imagenet_utils.decode_predictions(model.predict(image_batch))\n",
    "\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_class_names = [(lambda l, cl: cl[l][0]+cl[l][len(cl[l])-1])(label, class_names) for label in label_batch]\n",
    "three_digit_predictions = [(lambda prb: prb*100 if str(prb*100).replace(\",\", \".\").find(\".\") == -1 else int(str(prb*100).split(\".\")[0].replace(\"[\", \"\"))/100 )(prb) for prb in predicted_batch.numpy()]\n",
    "print(\"Labels Ids:           \", label_batch)\n",
    "print(\"predicted_class_names:           \",   predicted_class_names)\n",
    "print(\"three_digit_predictions: \", three_digit_predictions)\n",
    "# print(  (lambda x: x[x.index(max(x))]  )(three_digit_predictions) )\n",
    "print( three_digit_predictions[np.argmax(three_digit_predictions)] )\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9665be14e244f19f10b19628982837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='0', description='index', placeholder='current index goes here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda471ecd139496db18f28ade84a353d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Current', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da020169e414e2fa9bec4e78fa50e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1068afc4880d435295afcaf96f5ad85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48d6dbb74734004b4758cb89283f2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_path(\"images_backup/test/neutral\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490570594deb4d24a3f97e1ff1bcc7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Again', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef946fc05b04677905383de011660be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_at_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e1901e493748e1b0663d378b13fe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='0', description='index', placeholder='current index goes here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a805a6fed0414fb4bcc737286a6eae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Current', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff5c3621374c148f290ce4ea298045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e809bc05ae645c79eeb7954bec15ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c21123224c9403a961b757d39832a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_urls = [\n",
    "    \"https://i.ytimg.com/vi/yWI61kpFEAA/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDRAPwFXV09U5Eo-fhoUnh7FTbp1w\",\n",
    "    \"https://i.ytimg.com/vi/EiXQmeuHTOY/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBz-YYzwt-B30cjMrXYzm0PopCukg\",\n",
    "    \"https://i.ytimg.com/vi/poQXNp9ItL4/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLAyT3wtstrlzKYaC9sGf05ea66wmg\"\n",
    "]\n",
    "predict_url_batch(current_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_from_txt_file(src='validation-adult-save.txt', start=30, limit=40, break_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
