{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "    <li>Work  with imbalanced dataset => https://medium.com/geekculture/imbalanced-dataset-machine-learning-model-from-end-to-end-implementation-tensorflow-2-2-c48b5bc2eabc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  For macos AMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plaidml.keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "# os.environ[\"RUNFILES_DIR\"] = \"/Library/Frameworks/Python.framework/Versions/3.7/share/plaidml\"\n",
    "# os.environ[\"PLAIDML_NATIVE_PATH\"] = \"/Library/Frameworks/Python.framework/Versions/3.7/lib/libplaidml.dylib\"\n",
    "\n",
    "## to install => pip3 install plaidml.keras\n",
    "## to setup => plaidml-setup \n",
    "\n",
    "# plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32\n",
    "data_dir = \"images_new\"\n",
    "nsfw_classes_data = [{\"name\": \"general_nsfw\",\"index\": 5}]\n",
    "binary_classes_names = [\"adult\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/.DS_Store/.*\n",
      "zsh:1: no matches found: images_new/male_shirtless_or_underware/.*\n",
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/neutral/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "found 585 for class male_shirtless_or_underware\n",
      "found 1517 for class female_underwear\n",
      "found 2000 for class female_nudity\n",
      "found 2000 for class neutral\n",
      "found 739 for class female_swimwear\n",
      "found 2000 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless_or_underware/.*\n",
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/neutral/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 7067 images belonging to 6 classes.\n",
      "Found 1766 images belonging to 6 classes.\n",
      "class_weights =>  {0: 0.9338310145911096, 1: 0.8284130754439543, 2: 0.773781246465332, 3: 0.773781246465332, 4: 0.9164121705689402, 5: 0.773781246465332}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "num_samples = training_set.samples + validation_set.samples\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_2 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 7686      \n",
      "=================================================================\n",
      "Total params: 2,265,670\n",
      "Trainable params: 7,686\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "  # metrics=[\"accuracy\"]\n",
    "  metrics = [\n",
    "      \"categorical_accuracy\",\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "221/221 [==============================] - ETA: 0s - loss: 0.6270 - categorical_accuracy: 0.2582"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-16c789603569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m history = model.fit(training_set,\n\u001b[0m\u001b[1;32m     23\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0;31m# steps_per_epoch=steps_per_epoch,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1214\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_samples//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=3,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=30,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # callbacks=[model_checkpoint_callback],\n",
    "                    class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2de01e6e2514a9992e4db474bdff2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327f71dda40740baa1ef212d70da9652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26d4ab0c9b34132a7d9e63da9839014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mobilenet_v2_1642763870.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteDisconnected",
     "evalue": "Remote end closed connection without response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c513eb34c78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1624998901\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#export_path_keras = \"models/first-good-model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = tf.keras.models.load_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mexport_path_keras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# `custom_objects` tells keras how to load a `hub.KerasLayer`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         if (h5py is not None and\n\u001b[1;32m    200\u001b[0m             (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 201\u001b[0;31m           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0m\u001b[1;32m    202\u001b[0m                                                   compile)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    181\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m       layer = layer_module.deserialize(layer_config,\n\u001b[0m\u001b[1;32m    498\u001b[0m                                        custom_objects=custom_objects)\n\u001b[1;32m    499\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m           \u001b[0mdeserialized_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m       \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \"\"\"\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0m\u001b[1;32m     68\u001b[0m                                     self._lock_file_timeout_sec())\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading TF-Hub Module '%s'.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mdownload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;31m# Write module descriptor to capture information about which module was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;31m# downloaded by whom and when. The file stored at the same level as a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m       request = urllib.request.Request(\n\u001b[1;32m     62\u001b[0m           self._append_compressed_format_query(handle))\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       return resolver.DownloadManager(handle).download_and_uncompress(\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m_call_urlopen\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Overriding this method allows setting SSL context in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    535\u001b[0m                                   '_open', req)\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1390\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# Presumably, the server closed the connection before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    286\u001b[0m                                      \" response\")\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response"
     ]
    }
   ],
   "source": [
    "export_path_keras = \"models/inception_v3_1642705230.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Ids:            [1 1 2 2 2 3 1 0 0 2 2 2 5 4 0 3 4 2 2 4 3 2 2 4 0 1 2 2 0 2 2 1]\n",
      "predicted_class_names:            ['ge', 'ge', 'fy', 'fy', 'fy', 'mr', 'ge', 'ms', 'ms', 'fy', 'fy', 'fy', 'gw', 'fr', 'ms', 'mr', 'fr', 'fy', 'fy', 'fr', 'mr', 'fy', 'fy', 'fr', 'ms', 'ge', 'fy', 'fy', 'ms', 'fy', 'fy', 'ge']\n",
      "three_digit_predictions:  [0.01, 0.01, 0.05, 0.01, 0.05, 0.02, 0.03, 0.05, 0.03, 0.06, 0.01, 0.01, 0.02, 0.0, 0.02, 0.06, 0.08, 0.02, 0.01, 0.05, 0.01, 0.04, 0.09, 0.04, 0.8, 0.09, 0.06, 0.07, 0.01, 0.01, 0.01, 0.01]\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_class_names = [(lambda l, cl: cl[l][0]+cl[l][len(cl[l])-1])(label, class_names) for label in label_batch]\n",
    "three_digit_predictions = [(lambda prb: prb*100 if str(prb*100).replace(\",\", \".\").find(\".\") == -1 else int(str(prb*100).split(\".\")[0].replace(\"[\", \"\"))/100 )(prb) for prb in predicted_batch.numpy()]\n",
    "print(\"Labels Ids:           \", label_batch)\n",
    "print(\"predicted_class_names:           \",   predicted_class_names)\n",
    "print(\"three_digit_predictions: \", three_digit_predictions)\n",
    "# print(  (lambda x: x[x.index(max(x))]  )(three_digit_predictions) )\n",
    "print( three_digit_predictions[np.argmax(three_digit_predictions)] )\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_prediction(predicted_batch, get_images=False, image_set=[]):\n",
    "    # np_prediction = predicted_batch.numpy()\n",
    "    decoded_predictions = []\n",
    "    decoded_main_predictions_classes = []\n",
    "    max_indices = [(lambda pr: class_names[np.argmax(pr, axis=-1)])(predicton) for predicton in predicted_batch]\n",
    "    for count in range(0, len(predicted_batch)):\n",
    "        prd_btch = predicted_batch[count]\n",
    "        decoded_part = []\n",
    "        for i in range(0, num_classes):\n",
    "            decoded_prediction = {}\n",
    "            decoded_prediction[\"class_name\"] = class_names[i]\n",
    "            try:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i].numpy()\n",
    "            except Exception as e:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i]\n",
    "            decoded_prediction[\"precision\"] = np.sum(prd_btch[i]) / num_classes\n",
    "            \n",
    "            # decoded_prediction[\"count_index\"] = count\n",
    "        \n",
    "            if get_images:\n",
    "                decoded_prediction[\"image\"] = image_set[count]\n",
    "            decoded_part.append(decoded_prediction)\n",
    "        decoded_predictions.append(decoded_part)\n",
    "        \n",
    "        decoded_main_predictions_classes.append(decoded_part)\n",
    "    return decoded_predictions, decoded_main_predictions_classes, max_indices\n",
    "    \n",
    "\n",
    "decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(predicted_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be378aada2c4589ae6b9299544d052c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762bacff96744b24826ad7ce49e87947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90730227c5604ca5989b410af5aa449f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  4000\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def predict_single_image_from_path(path):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "        to_print  += \"{0} => {1} \\n \".format( class_names[i],  prediction[0][i] )\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [5 3 3 1 2 1 2 3 3 3 2 2 2 3 1 3 2 5 1 3 0 2 2 2 2 5 1 2 1 2 2 1]\n",
      "Predicted labels:  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n",
      "precisions :  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6213079"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_prediction(predictions):\n",
    "    binary_classes_index = []\n",
    "    predictions_probs = []\n",
    "    predictions_data = [] \n",
    "    numpy_predictions = predictions.numpy()\n",
    "    binary_class_names = []\n",
    "    for prediction in numpy_predictions:\n",
    "        nsfw_pred_sum = 0\n",
    "        binary_class_index = 0\n",
    "        for nsfw_classe_data in nsfw_classes_data:\n",
    "            nsfw_pred_sum += prediction[nsfw_classe_data[\"index\"]]\n",
    "\n",
    "        nsfw_pred_prob = nsfw_pred_sum / len(nsfw_classes_data)\n",
    "        \n",
    "        binary_class_index = 0 if nsfw_pred_prob > 0.5 else 1\n",
    "        binary_classes_index.append(nsfw_pred_prob)\n",
    "        predictions_probs.append(nsfw_pred_prob)\n",
    "\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "        binary_class_names.append(binary_classes_names[binary_class_index])\n",
    "    return np.array(binary_classes_index), np.array(predictions_probs), predictions, binary_class_names, predictions_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_ids, num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-8d65b8d6993c>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-ec2fc7a40586>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnumpy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbinary_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.9.6, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\n",
      "rootdir: /Users/macpro/Desktop/computer-science/projects/ai/holypics\n",
      "plugins: anyio-3.2.1, typeguard-2.13.3\n",
      "collected 0 items\n",
      "\n",
      "=============================== warnings summary ===============================\n",
      "../../../../../.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/_pytest/config/__init__.py:1114\n",
      "  /Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/_pytest/config/__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: typeguard\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "============================== 1 warning in 9.07s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.NO_TESTS_COLLECTED: 5>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "all_metrics = [\n",
    "    metrics.binary_accuracy,\n",
    "    metrics.categorical_accuracy,\n",
    "    metrics.mean_squared_error,\n",
    "    metrics.mean_absolute_error,\n",
    "    metrics.mean_absolute_percentage_error,\n",
    "    metrics.mean_squared_logarithmic_error,\n",
    "    metrics.squared_hinge,\n",
    "    metrics.hinge,\n",
    "    metrics.categorical_crossentropy,\n",
    "    metrics.binary_crossentropy,\n",
    "    metrics.poisson,\n",
    "    metrics.cosine_proximity,\n",
    "    # metrics.matthews_correlation,\n",
    "]\n",
    "\n",
    "all_sparse_metrics = [\n",
    "    metrics.sparse_categorical_accuracy,\n",
    "    metrics.sparse_categorical_crossentropy,\n",
    "]\n",
    "\n",
    "\n",
    "def test_metrics():\n",
    "    y_a = K.variable(np.random.random((6, 7)))\n",
    "    y_b = K.variable(np.random.random((6, 7)))\n",
    "    for metric in all_metrics:\n",
    "        output = metric(y_a, y_b)\n",
    "        assert K.eval(output).shape == ()\n",
    "\n",
    "\n",
    "def test_matthews_correlation():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.matthews_corrcoef\n",
    "    expected = -0.14907119849998601\n",
    "\n",
    "    actual = K.eval(metrics.matthews_correlation(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_precision():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.precision_score\n",
    "    expected = 0.40000000000000002\n",
    "\n",
    "    actual = K.eval(metrics.precision(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_recall():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.recall_score\n",
    "    expected = 0.2857142857142857\n",
    "\n",
    "    actual = K.eval(metrics.recall(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_fbeta_score():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.fbeta_score\n",
    "    expected = 0.30303030303030304\n",
    "\n",
    "    actual = K.eval(metrics.fbeta_score(y_true, y_pred, beta=2))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_fmeasure():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.f1_score\n",
    "    expected = 0.33333333333333331\n",
    "\n",
    "    actual = K.eval(metrics.fmeasure(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_sparse_metrics():\n",
    "    for metric in all_sparse_metrics:\n",
    "        y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n",
    "        y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n",
    "        assert K.eval(metric(y_a, y_b)).shape == ()\n",
    "\n",
    "\n",
    "def test_top_k_categorical_accuracy():\n",
    "    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n",
    "    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n",
    "    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=3))\n",
    "    assert success_result == 1\n",
    "    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=2))\n",
    "    assert partial_result == 0.5\n",
    "    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=1))\n",
    "    assert failure_result == 0\n",
    "\n",
    "\n",
    "\n",
    "pytest.main([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('holypics-SxDLhKSZ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
