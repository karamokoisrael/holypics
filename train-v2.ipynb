{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32\n",
    "data_dir = \"images_new\"\n",
    "nsfw_classes_data = [{\"name\": \"general_nsfw\",\"index\": 5}]\n",
    "binary_classes_names = [\"adult\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "found 954 for class general_not_nsfw_not_suggestive\n",
      "found 760 for class female_nudity\n",
      "found 926 for class female_swimwear\n",
      "found 588 for class male_underwear_or_shirtless\n",
      "found 860 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 3266 images belonging to 5 classes.\n",
      "Found 814 images belonging to 5 classes.\n",
      "class_weights =>  {0: 0.7666340508806262, 1: 0.8140900195694716, 2: 0.7734833659491194, 3: 0.8561643835616438, 4: 0.7896281800391389}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 2,103,301\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(num_classes, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "    layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5 # Keep it small when transfer learning\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#   optimizer=\"adam\",\n",
    "#   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#   # metrics=[\"accuracy\", f1_metric])\n",
    "#   metrics=[tfa.metrics.FBetaScore(num_classes=num_classes,average=\"micro\",threshold=0.9)]\n",
    "# )\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=macro_soft_f1,\n",
    "  metrics=[macro_f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "103/103 [==============================] - 169s 2s/step - loss: -0.2164 - macro_f1: 0.6570 - val_loss: -0.2944 - val_macro_f1: 0.6747\n",
      "Epoch 2/30\n",
      "103/103 [==============================] - 207s 2s/step - loss: -0.2986 - macro_f1: 0.6770 - val_loss: -0.3007 - val_macro_f1: 0.6765\n",
      "Epoch 3/30\n",
      "103/103 [==============================] - 210s 2s/step - loss: -0.3031 - macro_f1: 0.6758 - val_loss: -0.3046 - val_macro_f1: 0.6759\n",
      "Epoch 4/30\n",
      "103/103 [==============================] - 181s 2s/step - loss: -0.3029 - macro_f1: 0.6771 - val_loss: -0.3043 - val_macro_f1: 0.6780\n",
      "Epoch 5/30\n",
      "103/103 [==============================] - 188s 2s/step - loss: -0.3055 - macro_f1: 0.6782 - val_loss: -0.3025 - val_macro_f1: 0.6754\n",
      "Epoch 6/30\n",
      "103/103 [==============================] - 178s 2s/step - loss: -0.3042 - macro_f1: 0.6757 - val_loss: -0.3049 - val_macro_f1: 0.6768\n",
      "Epoch 7/30\n",
      "103/103 [==============================] - 191s 2s/step - loss: -0.3056 - macro_f1: 0.6756 - val_loss: -0.3024 - val_macro_f1: 0.6721\n",
      "Epoch 8/30\n",
      "103/103 [==============================] - 162s 2s/step - loss: -0.3051 - macro_f1: 0.6757 - val_loss: -0.3058 - val_macro_f1: 0.6773\n",
      "Epoch 9/30\n",
      "103/103 [==============================] - 171s 2s/step - loss: -0.3062 - macro_f1: 0.6754 - val_loss: -0.3038 - val_macro_f1: 0.6753\n",
      "Epoch 10/30\n",
      "103/103 [==============================] - 195s 2s/step - loss: -0.3048 - macro_f1: 0.6779 - val_loss: -0.3043 - val_macro_f1: 0.6766\n",
      "Epoch 11/30\n",
      "103/103 [==============================] - 204s 2s/step - loss: -0.3054 - macro_f1: 0.6783 - val_loss: -0.3042 - val_macro_f1: 0.6786\n"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch = num_classes//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_macro_f1', #'val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=3,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=EPOCHS,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['macro_f1']\n",
    "val_acc = history.history['val_macro_f1']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d4db339f86400d991ddb400feb95da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45e2da1a9d44b01bb9f3a51b457a5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c7cedaa88548549827fa7d19829400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/1642695022.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{}.h5\".format(int(t))\n",
    "#export_path_keras = \"models/{}.h5\".format(\"current-model-best-weight\")\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path_keras = \"models/1625175782.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [4 1 0 0 2 1 0 3 2 2 4 0 0 0 2 2 2 3 4 4 3 3 1 2 4 3 3 1 4 0 1 0]\n",
      "Predicted labels:  tf.Tensor(\n",
      "[[0.99996775 0.9999739  0.99990976 0.99996805 0.99996597]\n",
      " [0.9984994  0.9988238  0.997077   0.99959683 0.9994398 ]\n",
      " [0.9998214  0.99989223 0.99981165 0.99973965 0.99994886]\n",
      " [0.99997854 0.9999925  0.99998474 0.99997747 0.9999981 ]\n",
      " [0.99927497 0.998381   0.99932635 0.99929786 0.9991729 ]\n",
      " [0.9992649  0.9987934  0.9971427  0.9996257  0.999414  ]\n",
      " [0.9997635  0.9998049  0.9995513  0.99963343 0.99987996]\n",
      " [0.9999294  0.9999724  0.9998404  0.99990875 0.9999073 ]\n",
      " [0.9999636  0.9999788  0.9999265  0.999949   0.9999814 ]\n",
      " [0.99995595 0.9999361  0.9999429  0.9999339  0.9999671 ]\n",
      " [0.9994073  0.9994148  0.9992435  0.99906886 0.9996581 ]\n",
      " [0.9999969  0.99999523 0.999997   0.9999869  0.9999982 ]\n",
      " [0.9999838  0.9999966  0.9999635  0.99999344 0.99999195]\n",
      " [0.9999975  0.999988   0.9999951  0.9999945  0.9999982 ]\n",
      " [0.9999739  0.9999881  0.9999844  0.9999868  0.99999565]\n",
      " [0.9998435  0.9998927  0.9998559  0.998861   0.9998522 ]\n",
      " [0.9999942  0.9999957  0.9999969  0.9999994  0.9999989 ]\n",
      " [0.99987113 0.99992764 0.99980676 0.9998418  0.9999703 ]\n",
      " [0.99982274 0.99988043 0.9995563  0.9995967  0.99987006]\n",
      " [0.99998355 0.99998856 0.99998    0.99999106 0.99999213]\n",
      " [0.9999169  0.9995346  0.99986666 0.9999338  0.99988437]\n",
      " [0.999926   0.999944   0.99994016 0.9999385  0.9999882 ]\n",
      " [0.99892235 0.9987366  0.9959465  0.9991728  0.9993533 ]\n",
      " [0.99998224 0.99998885 0.99999034 0.9999829  0.9999901 ]\n",
      " [0.9992784  0.99947965 0.99861056 0.99937165 0.9995245 ]\n",
      " [0.9999994  0.9999995  0.99999917 0.9999996  1.        ]\n",
      " [0.99968594 0.99986136 0.99982107 0.9999598  0.99995387]\n",
      " [0.9999563  0.999966   0.99992067 0.99998033 0.9999752 ]\n",
      " [0.9997636  0.9994789  0.9996406  0.9993842  0.9997946 ]\n",
      " [0.9999794  0.9999956  0.9999684  0.99998045 0.9999944 ]\n",
      " [0.9993572  0.99943477 0.99926984 0.9996675  0.99970174]\n",
      " [0.9999986  0.9999995  0.99999875 0.99999934 0.99999964]], shape=(32, 5), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Shapes of all inputs must match: values[0].shape = [32] != values[1].shape = [32,5] [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-abc63d23c881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# print(\"precisions : \", precisions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m cfs_matrix = tf.math.confusion_matrix(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/ops/confusion_matrix.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(labels, predictions, num_classes, weights, dtype, name)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     values = (array_ops.ones_like(predictions, dtype)\n\u001b[1;32m    194\u001b[0m               if weights is None else weights)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1421\u001b[0m                        (axis, -expanded_num_dims, expanded_num_dims))\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6398\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6399\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6400\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6401\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6402\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [32] != values[1].shape = [32,5] [Op:Pack] name: stack"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "# predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_batch)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31abd97d5a974f7f9425e516aaa1fb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615966851db04241a6dd352b8e3f2f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f237ebf57b43e2a5a864b7d6fe9c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  3000\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def predict_single_image_from_path(path):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(numpy.array([image_resized]))\n",
    "    class_index = 0\n",
    "    max = 0\n",
    "    for i in range(0, len(prediction[0])):\n",
    "        if prediction[0][0] > max:\n",
    "            max =  prediction[0][i]\n",
    "            class_index = i\n",
    "    print(prediction[0])\n",
    "    # class_index = int(np.argmax(prediction[0], axis=0)) #numpy.where(prediction[0]== numpy.amax(prediction[0]))[0][0]\n",
    "    return class_index, class_names[class_index], Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        class_index, class_name, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(class_name)\n",
    "        display(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        class_index, class_name, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(class_name)\n",
    "        display(image)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [5 3 3 1 2 1 2 3 3 3 2 2 2 3 1 3 2 5 1 3 0 2 2 2 2 5 1 2 1 2 2 1]\n",
      "Predicted labels:  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n",
      "precisions :  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6213079"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJ10lEQVR4nO3d24tdhR3F8bUcE02jNmBTCZlgfLAWsVTLNA/VFhqwjRe0T0VBn4QgVIi0IEqf/AfEl74MKm3RGgQVRG1tqBEJaJJJjJckWkKwmChEK6IJmpurD3MCU4mZfU72nn348f3A4FwOJwvJN/tc5pztJAJQxzl9DwDQLqIGiiFqoBiiBoohaqCYc7u40sXnLMmScy/q4qpHkuPH+54AtOorHdGxHPXpftZJ1EvOvUg/u+S2Lq56JCcOftj3BKBVW/Ovb/0ZN7+BYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYhpFbXud7fds77N9f9ejAIxu3qhtT0j6k6QbJF0p6XbbV3Y9DMBomhyp10jal2R/kmOSNkq6tdtZAEbVJOqVkj6Y8/WBwff+j+31tmdszxz7+su29gEYUmsPlCWZTjKVZGrxOUvauloAQ2oS9UFJq+Z8PTn4HoAx1CTq7ZIut32Z7cWSbpP0XLezAIxq3jceTHLC9j2SXpI0IemxJLs7XwZgJI3eTTTJi5Je7HgLgBbwG2VAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0+gFHcP6asUi7fnjZBdXPZIf3P1h3xOABcORGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBooZt6obT9m+5DtdxZiEICz0+RI/WdJ6zreAaAl80ad5FVJny7AFgAtaO0+te31tmdsz5w8fKStqwUwpNaiTjKdZCrJ1MQFS9u6WgBD4tFvoBiiBopp8pTWk5Jek3SF7QO27+p+FoBRzfu+30luX4ghANrBzW+gGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKmfcFHaP40bJPtO2W6S6ueiS/vvvqvicAC4YjNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFNDlB3irbm23vsb3b9oaFGAZgNE1eT31C0h+S7LR9oaQdtjcl2dPxNgAjmPdIneSjJDsHn38haa+klV0PAzCaoe5T214t6RpJW0/zs/W2Z2zPfPzfky3NAzCsxlHbvkDS05LuTfL5N3+eZDrJVJKp5RdPtLkRwBAaRW17kWaDfiLJM91OAnA2mjz6bUmPStqb5KHuJwE4G02O1NdKulPSWtu7Bh83drwLwIjmfUoryRZJXoAtAFrAb5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQTJP3KBvaO4cv1g+33NnFVY/kUr3d9wRgwXCkBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqCYJme9PN/2Nttv2t5t+8GFGAZgNE1eT31U0tokhwfnqd5i++9JXu94G4ARNDnrZSQdHny5aPCRLkcBGF2j+9S2J2zvknRI0qYkW09zmfW2Z2zPnPz8SMszATTVKOokJ5NcLWlS0hrbV53mMtNJppJMTVy0tOWZAJoa6tHvJJ9J2ixpXSdrAJy1Jo9+L7e9bPD5EknXS3q3410ARtTk0e8Vkv5ie0Kz/wg8leT5bmcBGFWTR7/fknTNAmwB0AJ+owwohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFimrxKa2iL93+pS3/7dhdXDWAeHKmBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKZx1IMTz79hm5PjAWNsmCP1Bkl7uxoCoB2NorY9KekmSY90OwfA2Wp6pH5Y0n2Svv62C9heb3vG9sxxHW1jG4ARzBu17ZslHUqy40yXSzKdZCrJ1CKd19pAAMNpcqS+VtIttt+XtFHSWtuPd7oKwMjmjTrJA0kmk6yWdJukl5Pc0fkyACPheWqgmKHeIjjJK5Je6WQJgFZwpAaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKGepVWk3lu9/R0Z//tIurHsl5L2zvewKwYDhSA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVBMo5deDs5N/YWkk5JOJJnqchSA0Q3zeupfJvmksyUAWsHNb6CYplFH0j9t77C9/nQXsL3e9oztmePHjrS3EMBQmt78vi7JQdvfl7TJ9rtJXp17gSTTkqYl6cJlk2l5J4CGGh2pkxwc/PeQpGclrelyFIDRzRu17aW2Lzz1uaRfSXqn62EARtPk5vclkp61feryf0vyj05XARjZvFEn2S/pxwuwBUALeEoLKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYpy0/34Gtj+W9J8Wrup7ksbpfdHYc2bjtkcav01t7bk0yfLT/aCTqNtie2ac3rmUPWc2bnuk8du0EHu4+Q0UQ9RAMeMe9XTfA76BPWc2bnuk8dvU+Z6xvk8NYHjjfqQGMCSiBooZy6htr7P9nu19tu8fgz2P2T5keyzeGtn2Ktubbe+xvdv2hp73nG97m+03B3se7HPPKbYnbL9h+/m+t0izJ5q0/bbtXbZnOvtzxu0+te0JSf+WdL2kA5K2S7o9yZ4eN/1C0mFJf01yVV875uxZIWlFkp2D92TfIek3ff0/8uz7Ry9Nctj2IklbJG1I8nofe+bs+r2kKUkXJbm5zy2DPe9Lmur6RJPjeKReI2lfkv1JjknaKOnWPgcNTjH0aZ8b5kryUZKdg8+/kLRX0soe9yTJ4cGXiwYfvR4tbE9KuknSI33u6MM4Rr1S0gdzvj6gHv/CjjvbqyVdI2lrzzsmbO+SdEjSpiS97pH0sKT7JH3d84655j3RZBvGMWo0ZPsCSU9LujfJ531uSXIyydWSJiWtsd3b3RTbN0s6lGRHXxu+xXVJfiLpBkm/G9yta904Rn1Q0qo5X08Ovoc5Bvddn5b0RJJn+t5zSpLPJG2WtK7HGddKumVwH3ajpLW2H+9xj6SFO9HkOEa9XdLlti+zvVjSbZKe63nTWBk8MPWopL1JHhqDPcttLxt8vkSzD3K+29eeJA8kmUyyWrN/f15Ockdfe6SFPdHk2EWd5ISkeyS9pNkHgJ5KsrvPTbaflPSapCtsH7B9V597NHskulOzR6Bdg48be9yzQtJm229p9h/lTUnG4mmkMXKJpC2235S0TdILXZ1ocuye0gJwdsbuSA3g7BA1UAxRA8UQNVAMUQPFEDVQDFEDxfwPbCJcXaIvIYgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_prediction(predictions):\n",
    "    binary_classes_index = []\n",
    "    predictions_probs = []\n",
    "    predictions_data = [] \n",
    "    numpy_predictions = predictions.numpy()\n",
    "    binary_class_names = []\n",
    "    for prediction in numpy_predictions:\n",
    "        nsfw_pred_sum = 0\n",
    "        binary_class_index = 0\n",
    "        for nsfw_classe_data in nsfw_classes_data:\n",
    "            nsfw_pred_sum += prediction[nsfw_classe_data[\"index\"]]\n",
    "\n",
    "        nsfw_pred_prob = nsfw_pred_sum / len(nsfw_classes_data)\n",
    "        \n",
    "        binary_class_index = 0 if nsfw_pred_prob > 0.5 else 1\n",
    "        binary_classes_index.append(nsfw_pred_prob)\n",
    "        predictions_probs.append(nsfw_pred_prob)\n",
    "\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "        binary_class_names.append(binary_classes_names[binary_class_index])\n",
    "    return np.array(binary_classes_index), np.array(predictions_probs), predictions, binary_class_names, predictions_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_ids, num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-8d65b8d6993c>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-ec2fc7a40586>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnumpy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbinary_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('holypics-SxDLhKSZ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
