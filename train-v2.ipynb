{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "    <li>Work  with imbalanced dataset => https://medium.com/geekculture/imbalanced-dataset-machine-learning-model-from-end-to-end-implementation-tensorflow-2-2-c48b5bc2eabc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  For macos AMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plaidml.keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "# os.environ[\"RUNFILES_DIR\"] = \"/Library/Frameworks/Python.framework/Versions/3.7/share/plaidml\"\n",
    "# os.environ[\"PLAIDML_NATIVE_PATH\"] = \"/Library/Frameworks/Python.framework/Versions/3.7/lib/libplaidml.dylib\"\n",
    "\n",
    "## to install => pip3 install plaidml.keras\n",
    "## to setup => plaidml-setup \n",
    "\n",
    "# plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32\n",
    "data_dir = \"images_new\"\n",
    "nsfw_classes_data = [{\"name\": \"general_nsfw\",\"index\": 5}]\n",
    "binary_classes_names = [\"adult\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/sex_toys/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "found 429 for class male_shirtless\n",
      "found 2000 for class general_not_nsfw_not_suggestive\n",
      "found 1517 for class female_underwear\n",
      "found 2000 for class female_nudity\n",
      "found 156 for class male_underwear\n",
      "found 739 for class female_swimwear\n",
      "found 472 for class sex_toys\n",
      "found 2000 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/sex_toys/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 7440 images belonging to 8 classes.\n",
      "Found 1858 images belonging to 8 classes.\n",
      "class_weights =>  {0: 0.9539353591753463, 1: 0.7852464297218942, 2: 0.8371094169440567, 3: 0.7852464297218942, 4: 0.9832492215183077, 5: 0.9206485557822399, 6: 0.949318157414367, 7: 0.7852464297218942}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "num_samples = training_set.samples + validation_set.samples\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 10248     \n",
      "=================================================================\n",
      "Total params: 2,268,232\n",
      "Trainable params: 10,248\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "  # metrics=[\"accuracy\"]\n",
    "  metrics = [\n",
    "      \"categorical_accuracy\",\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 17/233 [=>............................] - ETA: 1:53 - loss: 1.6484 - categorical_accuracy: 0.2077"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_samples//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=3,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=30,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2de01e6e2514a9992e4db474bdff2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327f71dda40740baa1ef212d70da9652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26d4ab0c9b34132a7d9e63da9839014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mobilenet_v2_1642763870.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteDisconnected",
     "evalue": "Remote end closed connection without response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c513eb34c78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1624998901\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#export_path_keras = \"models/first-good-model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = tf.keras.models.load_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mexport_path_keras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# `custom_objects` tells keras how to load a `hub.KerasLayer`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         if (h5py is not None and\n\u001b[1;32m    200\u001b[0m             (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 201\u001b[0;31m           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0m\u001b[1;32m    202\u001b[0m                                                   compile)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    181\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m       layer = layer_module.deserialize(layer_config,\n\u001b[0m\u001b[1;32m    498\u001b[0m                                        custom_objects=custom_objects)\n\u001b[1;32m    499\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m           \u001b[0mdeserialized_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m       \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \"\"\"\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0m\u001b[1;32m     68\u001b[0m                                     self._lock_file_timeout_sec())\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading TF-Hub Module '%s'.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mdownload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;31m# Write module descriptor to capture information about which module was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;31m# downloaded by whom and when. The file stored at the same level as a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m       request = urllib.request.Request(\n\u001b[1;32m     62\u001b[0m           self._append_compressed_format_query(handle))\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       return resolver.DownloadManager(handle).download_and_uncompress(\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m_call_urlopen\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Overriding this method allows setting SSL context in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    535\u001b[0m                                   '_open', req)\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1390\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# Presumably, the server closed the connection before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    286\u001b[0m                                      \" response\")\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response"
     ]
    }
   ],
   "source": [
    "export_path_keras = \"models/inception_v3_1642705230.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Ids:            [1 1 2 2 2 3 1 0 0 2 2 2 5 4 0 3 4 2 2 4 3 2 2 4 0 1 2 2 0 2 2 1]\n",
      "predicted_class_names:            ['ge', 'ge', 'fy', 'fy', 'fy', 'mr', 'ge', 'ms', 'ms', 'fy', 'fy', 'fy', 'gw', 'fr', 'ms', 'mr', 'fr', 'fy', 'fy', 'fr', 'mr', 'fy', 'fy', 'fr', 'ms', 'ge', 'fy', 'fy', 'ms', 'fy', 'fy', 'ge']\n",
      "three_digit_predictions:  [0.01, 0.01, 0.05, 0.01, 0.05, 0.02, 0.03, 0.05, 0.03, 0.06, 0.01, 0.01, 0.02, 0.0, 0.02, 0.06, 0.08, 0.02, 0.01, 0.05, 0.01, 0.04, 0.09, 0.04, 0.8, 0.09, 0.06, 0.07, 0.01, 0.01, 0.01, 0.01]\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_class_names = [(lambda l, cl: cl[l][0]+cl[l][len(cl[l])-1])(label, class_names) for label in label_batch]\n",
    "three_digit_predictions = [(lambda prb: prb*100 if str(prb*100).replace(\",\", \".\").find(\".\") == -1 else int(str(prb*100).split(\".\")[0].replace(\"[\", \"\"))/100 )(prb) for prb in predicted_batch.numpy()]\n",
    "print(\"Labels Ids:           \", label_batch)\n",
    "print(\"predicted_class_names:           \",   predicted_class_names)\n",
    "print(\"three_digit_predictions: \", three_digit_predictions)\n",
    "# print(  (lambda x: x[x.index(max(x))]  )(three_digit_predictions) )\n",
    "print( three_digit_predictions[np.argmax(three_digit_predictions)] )\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_prediction(predicted_batch, get_images=False, image_set=[]):\n",
    "    # np_prediction = predicted_batch.numpy()\n",
    "    decoded_predictions = []\n",
    "    decoded_main_predictions_classes = []\n",
    "    max_indices = [(lambda pr: class_names[np.argmax(pr, axis=-1)])(predicton) for predicton in predicted_batch]\n",
    "    for count in range(0, len(predicted_batch)):\n",
    "        prd_btch = predicted_batch[count]\n",
    "        decoded_part = []\n",
    "        for i in range(0, num_classes):\n",
    "            decoded_prediction = {}\n",
    "            decoded_prediction[\"class_name\"] = class_names[i]\n",
    "            try:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i].numpy()\n",
    "            except Exception as e:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i]\n",
    "            decoded_prediction[\"precision\"] = np.sum(prd_btch[i]) / num_classes\n",
    "            \n",
    "            # decoded_prediction[\"count_index\"] = count\n",
    "        \n",
    "            if get_images:\n",
    "                decoded_prediction[\"image\"] = image_set[count]\n",
    "            decoded_part.append(decoded_prediction)\n",
    "        decoded_predictions.append(decoded_part)\n",
    "        \n",
    "        decoded_main_predictions_classes.append(decoded_part)\n",
    "    return decoded_predictions, decoded_main_predictions_classes, max_indices\n",
    "    \n",
    "\n",
    "decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(predicted_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be378aada2c4589ae6b9299544d052c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762bacff96744b24826ad7ce49e87947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90730227c5604ca5989b410af5aa449f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  4000\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def predict_single_image_from_path(path):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "        to_print  += \"{0} => {1} \\n \".format( class_names[i],  prediction[0][i] )\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [5 3 3 1 2 1 2 3 3 3 2 2 2 3 1 3 2 5 1 3 0 2 2 2 2 5 1 2 1 2 2 1]\n",
      "Predicted labels:  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n",
      "precisions :  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6213079"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_prediction(predictions):\n",
    "    binary_classes_index = []\n",
    "    predictions_probs = []\n",
    "    predictions_data = [] \n",
    "    numpy_predictions = predictions.numpy()\n",
    "    binary_class_names = []\n",
    "    for prediction in numpy_predictions:\n",
    "        nsfw_pred_sum = 0\n",
    "        binary_class_index = 0\n",
    "        for nsfw_classe_data in nsfw_classes_data:\n",
    "            nsfw_pred_sum += prediction[nsfw_classe_data[\"index\"]]\n",
    "\n",
    "        nsfw_pred_prob = nsfw_pred_sum / len(nsfw_classes_data)\n",
    "        \n",
    "        binary_class_index = 0 if nsfw_pred_prob > 0.5 else 1\n",
    "        binary_classes_index.append(nsfw_pred_prob)\n",
    "        predictions_probs.append(nsfw_pred_prob)\n",
    "\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "        binary_class_names.append(binary_classes_names[binary_class_index])\n",
    "    return np.array(binary_classes_index), np.array(predictions_probs), predictions, binary_class_names, predictions_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_ids, num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-8d65b8d6993c>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-ec2fc7a40586>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnumpy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbinary_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.9.6, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\n",
      "rootdir: /Users/macpro/Desktop/computer-science/projects/ai/holypics\n",
      "plugins: anyio-3.2.1, typeguard-2.13.3\n",
      "collected 0 items\n",
      "\n",
      "=============================== warnings summary ===============================\n",
      "../../../../../.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/_pytest/config/__init__.py:1114\n",
      "  /Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/_pytest/config/__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: typeguard\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "============================== 1 warning in 9.07s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.NO_TESTS_COLLECTED: 5>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "all_metrics = [\n",
    "    metrics.binary_accuracy,\n",
    "    metrics.categorical_accuracy,\n",
    "    metrics.mean_squared_error,\n",
    "    metrics.mean_absolute_error,\n",
    "    metrics.mean_absolute_percentage_error,\n",
    "    metrics.mean_squared_logarithmic_error,\n",
    "    metrics.squared_hinge,\n",
    "    metrics.hinge,\n",
    "    metrics.categorical_crossentropy,\n",
    "    metrics.binary_crossentropy,\n",
    "    metrics.poisson,\n",
    "    metrics.cosine_proximity,\n",
    "    # metrics.matthews_correlation,\n",
    "]\n",
    "\n",
    "all_sparse_metrics = [\n",
    "    metrics.sparse_categorical_accuracy,\n",
    "    metrics.sparse_categorical_crossentropy,\n",
    "]\n",
    "\n",
    "\n",
    "def test_metrics():\n",
    "    y_a = K.variable(np.random.random((6, 7)))\n",
    "    y_b = K.variable(np.random.random((6, 7)))\n",
    "    for metric in all_metrics:\n",
    "        output = metric(y_a, y_b)\n",
    "        assert K.eval(output).shape == ()\n",
    "\n",
    "\n",
    "def test_matthews_correlation():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.matthews_corrcoef\n",
    "    expected = -0.14907119849998601\n",
    "\n",
    "    actual = K.eval(metrics.matthews_correlation(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_precision():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.precision_score\n",
    "    expected = 0.40000000000000002\n",
    "\n",
    "    actual = K.eval(metrics.precision(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_recall():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.recall_score\n",
    "    expected = 0.2857142857142857\n",
    "\n",
    "    actual = K.eval(metrics.recall(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_fbeta_score():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.fbeta_score\n",
    "    expected = 0.30303030303030304\n",
    "\n",
    "    actual = K.eval(metrics.fbeta_score(y_true, y_pred, beta=2))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_fmeasure():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.f1_score\n",
    "    expected = 0.33333333333333331\n",
    "\n",
    "    actual = K.eval(metrics.fmeasure(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_sparse_metrics():\n",
    "    for metric in all_sparse_metrics:\n",
    "        y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n",
    "        y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n",
    "        assert K.eval(metric(y_a, y_b)).shape == ()\n",
    "\n",
    "\n",
    "def test_top_k_categorical_accuracy():\n",
    "    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n",
    "    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n",
    "    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=3))\n",
    "    assert success_result == 1\n",
    "    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=2))\n",
    "    assert partial_result == 0.5\n",
    "    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=1))\n",
    "    assert failure_result == 0\n",
    "\n",
    "\n",
    "\n",
    "pytest.main([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('holypics-SxDLhKSZ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
