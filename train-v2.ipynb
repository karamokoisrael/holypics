{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32\n",
    "data_dir = \"images_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/male_underware/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "found 435 for class male_shirtless\n",
      "found 2107 for class general_not_nsfw_not_suggestive\n",
      "found 153 for class male_underware\n",
      "found 798 for class female_nudity\n",
      "found 927 for class female_swimwear\n",
      "found 933 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/male_underware/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 4279 images belonging to 6 classes.\n",
      "Found 1066 images belonging to 6 classes.\n",
      "{0: 0.9187371567345414, 1: 0.6063889407808705, 2: 0.9714178965066318, 3: 0.8509247151130207, 4: 0.8268260788342985, 5: 0.825705212030637}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_9 (KerasLayer)   (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 21,815,078\n",
      "Trainable params: 12,294\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    # layers.Dense(num_classes, activation=\"softmax\")\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#   metrics=[tf.keras.metrics.AUC(name='prc', curve='PR')],\n",
    "#   # optimizer=tf.keras.optimizers.RMSprop(lr=0.01), \n",
    "#   optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4),\n",
    "#   loss = tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "\n",
    "# model.compile(\n",
    "#   optimizer=\"adam\",#tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#   metrics=[\"accuracy\"]\n",
    "#   )\n",
    "\n",
    "  #, tfa.metrics.F1Score(num_classes=num_classes, threshold=0.5)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "  optimizer=\"adam\",\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 15/134 [==>...........................] - ETA: 1:18 - loss: 1.3293 - accuracy: 0.4500"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10 #30\n",
    "# steps_per_epoch = num_classes//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy', #'val_prc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=3,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=EPOCHS,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/male_shirtless/.*\n",
      "zsh:1: no matches found: images_new/.DS_Store/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800a38fee74a414980b57e151d9decb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6affca327a443069d8010fc90e98106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeecf2d58e1423291ef83a225414bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/1642608117.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{}.h5\".format(int(t))\n",
    "#export_path_keras = \"models/{}.h5\".format(\"current-model-best-weight\")\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path_keras = \"models/1625175782.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor =>  tf.Tensor([-3.8986447 -4.897027   4.175486  -4.6594253 -6.8515882 -9.304119 ], shape=(6,), dtype=float32)\n",
      "numpy =>  [-3.8986447 -4.897027   4.175486  -4.6594253 -6.8515882 -9.304119 ]\n",
      "prediction_data =>  {'male_shirtless': -3.8986447, 'general_not_nsfw_not_suggestive': -4.897027, 'male_underware': 4.175486, 'female_nudity': -4.6594253, 'female_swimwear': -6.8515882, 'general_nsfw': -9.304119}\n"
     ]
    }
   ],
   "source": [
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    predictions_data = [] \n",
    "\n",
    "    numpy_predictions = predictions.numpy()\n",
    "    \n",
    "    for prediction in numpy_predictions:\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "\n",
    "    print(\"tensor => \", predictions[0])\n",
    "    print(\"numpy => \", numpy_predictions[0])\n",
    "    print(\"prediction_data => \", predictions_data[0])\n",
    "   \n",
    "    \n",
    "            # print()\n",
    "    #     result = 0 if prediction < 0.5 else 1\n",
    "    #     precision = calculate_average(prediction)\n",
    "    #     decoded_class_index.append(result)\n",
    "    #     decode_prediction_precision.append(precision)\n",
    "    # return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "decode_prediction(predicted_batch)\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )\n",
    "# 0.6213079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(class_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-98eabefdb284>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-be7be04ac734>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('holypics-SxDLhKSZ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
