{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {zZZ\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "    <li>Work  with imbalanced dataset => https://medium.com/geekculture/imbalanced-dataset-machine-learning-model-from-end-to-end-implementation-tensorflow-2-2-c48b5bc2eabc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T17:40:35.099556Z",
     "iopub.status.busy": "2022-01-27T17:40:35.099269Z",
     "iopub.status.idle": "2022-01-27T17:40:58.703371Z",
     "shell.execute_reply": "2022-01-27T17:40:58.702798Z",
     "shell.execute_reply.started": "2022-01-27T17:40:35.099527Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /opt/conda/envs/saturn/lib/python3.9/site-packages (from -r requirements-notebook.txt (line 1)) (4.5.5.62)\n",
      "Requirement already satisfied: tensorflow_hub in /opt/conda/envs/saturn/lib/python3.9/site-packages (from -r requirements-notebook.txt (line 2)) (0.12.0)\n",
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow_gpu-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/saturn/lib/python3.9/site-packages (from -r requirements-notebook.txt (line 4)) (1.0.2)\n",
      "Requirement already satisfied: tensorflow_addons in /opt/conda/envs/saturn/lib/python3.9/site-packages (from -r requirements-notebook.txt (line 5)) (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from opencv-python-headless->-r requirements-notebook.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow_hub->-r requirements-notebook.txt (line 2)) (3.19.1)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[K     |████████████████████████████████| 463 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.1.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.23.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.12.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.37.1)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.34.1)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from scikit-learn->-r requirements-notebook.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from scikit-learn->-r requirements-notebook.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from scikit-learn->-r requirements-notebook.txt (line 4)) (1.7.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorflow_addons->-r requirements-notebook.txt (line 5)) (2.13.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (58.0.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/saturn/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu->-r requirements-notebook.txt (line 3)) (3.1.0)\n",
      "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-estimator, libclang, keras, tensorflow-gpu\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed keras-2.7.0 libclang-13.0.0 tensorflow-estimator-2.7.0 tensorflow-gpu-2.7.0 tensorflow-io-gcs-filesystem-0.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements-notebook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T17:38:43.639206Z",
     "iopub.status.busy": "2022-01-27T17:38:43.638895Z",
     "iopub.status.idle": "2022-01-27T17:38:46.422808Z",
     "shell.execute_reply": "2022-01-27T17:38:46.422237Z",
     "shell.execute_reply.started": "2022-01-27T17:38:43.639180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.5.0\n",
      "Uninstalling tensorflow-2.5.0:\n",
      "  Successfully uninstalled tensorflow-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://storage.googleapis.com/kaggle-data-sets/1885605/3083816/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220127%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220127T102630Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=173a91a8aaced400cb01f48648c997f0aac12cdbd597c4a48ae4089925004936e9e3688a71f69507c76736a13a5553f862af96fff8de4c9bc9ffea29d1ae8c2e976924e3eb329d6fd7f176b8dfb1b33cfb9fa3b000e3e916d15e55a72d9b8d8be048a808fb8fad044e59a79ad88a80fce3d8fb2fe98665dbe6be336ba8dd182f0a2b14bfa8efe46e201d94d9947d30d2f48f82925bbcaab47ab6a0b88dc7617a94704b0c84999388b312644910e369805627e3f95ce6c0f2103da2252947f46f954e20c0360c47763139ad27530fccc3d87bed953ef53005237c8a262a733446958bcf9d79196bd131c8bbfb07a8c67ce1062082221137bc336c25c0f4431fa7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T10:28:01.595074Z",
     "iopub.status.busy": "2022-01-27T10:28:01.594852Z",
     "iopub.status.idle": "2022-01-27T10:28:02.334565Z",
     "shell.execute_reply": "2022-01-27T10:28:02.333899Z",
     "shell.execute_reply.started": "2022-01-27T10:28:01.595052Z"
    }
   },
   "outputs": [],
   "source": [
    "!mv archive.zip* archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "#from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "# print(tf.config.list_pZZzhysical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "EPOCHS=15\n",
    "PATIENCE=3\n",
    "LR = 1e-5\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 256#32\n",
    "data_dir = \"images_new\"\n",
    "nsfw_classes_data = [{\"name\": \"general_nsfw\",\"index\": 5}]\n",
    "binary_classes_names = [\"adult\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/neutral/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "found 1500 for class female_underwear\n",
      "found 1500 for class female_nudity\n",
      "found 1500 for class neutral\n",
      "found 739 for class female_swimwear\n",
      "found 1500 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/neutral/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 5389 images belonging to 5 classes.\n",
      "Found 1346 images belonging to 5 classes.\n",
      "class_weights =>  {0: 0.7774150467428402, 1: 0.7774150467428402, 2: 0.7774150467428402, 3: 0.8903398130286393, 4: 0.7774150467428402}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "num_samples = training_set.samples + validation_set.samples\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_3 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 1024)              1311744   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 3,574,853\n",
      "Trainable params: 1,316,869\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "    layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "])\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(num_classes, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which we predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#   optimizer=\"adam\",\n",
    "# #   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#     loss=tf.keras.losses.KLDivergence \n",
    "    \n",
    "# #   metrics=[\"accuracy\"]\n",
    "#   metrics = [\"categorical_accuracy\",]\n",
    "# )\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=macro_soft_f1,\n",
    "  metrics=[macro_f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_samples//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=PATIENCE,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=EPOCHS,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # callbacks=[model_checkpoint_callback],\n",
    "                    class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/neutral/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab81de53c504165b223242ed81f0a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db333386212048839bd2235f95a7a23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d259d2876974844bf0038042895f586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mobilenet_v2_1643727661.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "RemoteDisconnected",
     "evalue": "Remote end closed connection without response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c513eb34c78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1624998901\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#export_path_keras = \"models/first-good-model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = tf.keras.models.load_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mexport_path_keras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# `custom_objects` tells keras how to load a `hub.KerasLayer`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         if (h5py is not None and\n\u001b[1;32m    200\u001b[0m             (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 201\u001b[0;31m           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0m\u001b[1;32m    202\u001b[0m                                                   compile)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    181\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m       layer = layer_module.deserialize(layer_config,\n\u001b[0m\u001b[1;32m    498\u001b[0m                                        custom_objects=custom_objects)\n\u001b[1;32m    499\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m           \u001b[0mdeserialized_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m       \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \"\"\"\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0m\u001b[1;32m     68\u001b[0m                                     self._lock_file_timeout_sec())\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading TF-Hub Module '%s'.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mdownload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;31m# Write module descriptor to capture information about which module was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;31m# downloaded by whom and when. The file stored at the same level as a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m       request = urllib.request.Request(\n\u001b[1;32m     62\u001b[0m           self._append_compressed_format_query(handle))\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       return resolver.DownloadManager(handle).download_and_uncompress(\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m_call_urlopen\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Overriding this method allows setting SSL context in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    535\u001b[0m                                   '_open', req)\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1390\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# Presumably, the server closed the connection before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    286\u001b[0m                                      \" response\")\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response"
     ]
    }
   ],
   "source": [
    "export_path_keras = \"models/inception_v3_1642705230.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Ids:            [0 1 0 2 0 0 4 1]\n",
      "predicted_class_names:            ['fr', 'fy', 'fr', 'nl', 'fr', 'fr', 'gw', 'fy']\n",
      "three_digit_predictions:  [0.08, 0.19, 0.09, 0.56, 0.02, 0.05, 0.05, 0.04]\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_class_names = [(lambda l, cl: cl[l][0]+cl[l][len(cl[l])-1])(label, class_names) for label in label_batch]\n",
    "three_digit_predictions = [(lambda prb: prb*100 if str(prb*100).replace(\",\", \".\").find(\".\") == -1 else int(str(prb*100).split(\".\")[0].replace(\"[\", \"\"))/100 )(prb) for prb in predicted_batch.numpy()]\n",
    "print(\"Labels Ids:           \", label_batch)\n",
    "print(\"predicted_class_names:           \",   predicted_class_names)\n",
    "print(\"three_digit_predictions: \", three_digit_predictions)\n",
    "# print(  (lambda x: x[x.index(max(x))]  )(three_digit_predictions) )\n",
    "print( three_digit_predictions[np.argmax(three_digit_predictions)] )\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_prediction(predicted_batch, get_images=False, image_set=[]):\n",
    "    # np_prediction = predicted_batch.numpy()\n",
    "    decoded_predictions = []\n",
    "    decoded_main_predictions_classes = []\n",
    "    max_indices = [(lambda pr: class_names[np.argmax(pr, axis=-1)])(predicton) for predicton in predicted_batch]\n",
    "    for count in range(0, len(predicted_batch)):\n",
    "        prd_btch = predicted_batch[count]\n",
    "        decoded_part = []\n",
    "        for i in range(0, num_classes):\n",
    "            decoded_prediction = {}\n",
    "            decoded_prediction[\"class_name\"] = class_names[i]\n",
    "            try:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i].numpy()\n",
    "            except Exception as e:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i]\n",
    "            decoded_prediction[\"precision\"] = np.sum(prd_btch[i]) / num_classes\n",
    "            \n",
    "            # decoded_prediction[\"count_index\"] = count\n",
    "        \n",
    "            if get_images:\n",
    "                decoded_prediction[\"image\"] = image_set[count]\n",
    "            decoded_part.append(decoded_prediction)\n",
    "        decoded_predictions.append(decoded_part)\n",
    "        \n",
    "        decoded_main_predictions_classes.append(decoded_part)\n",
    "    return decoded_predictions, decoded_main_predictions_classes, max_indices\n",
    "    \n",
    "\n",
    "decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(predicted_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/female_underwear/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/neutral/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeac3e3ee2474629b090a42125486b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4ca904d83e40578b1a14925002bf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff2e7ea689c4277ac98fabde60300a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  4000\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def predict_single_image_from_path(path):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0,3]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "       \n",
    "        to_print  += \"{0} => {1} \\n \".format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9e2f1809a386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mpredicted_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mpredicted_ids\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_class_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-9e2f1809a386>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbinary_class_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnsfw_classe_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnsfw_classes_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mnsfw_pred_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnsfw_classe_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnsfw_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnsfw_pred_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsfw_classes_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "def decode_prediction(predictions):\n",
    "    binary_classes_index = []\n",
    "    predictions_probs = []\n",
    "    predictions_data = [] \n",
    "    numpy_predictions = predictions.numpy()\n",
    "    binary_class_names = []\n",
    "    for prediction in numpy_predictions:\n",
    "        nsfw_pred_sum = 0\n",
    "        binary_class_index = 0\n",
    "        for nsfw_classe_data in nsfw_classes_data:\n",
    "            nsfw_pred_sum += prediction[nsfw_classe_data[\"index\"]]\n",
    "\n",
    "        nsfw_pred_prob = nsfw_pred_sum / len(nsfw_classes_data)\n",
    "        \n",
    "        binary_class_index = 0 if nsfw_pred_prob > 0.5 else 1\n",
    "        binary_classes_index.append(nsfw_pred_prob)\n",
    "        predictions_probs.append(nsfw_pred_prob)\n",
    "\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "        binary_class_names.append(binary_classes_names[binary_class_index])\n",
    "    return np.array(binary_classes_index), np.array(predictions_probs), predictions, binary_class_names, predictions_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_ids, num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-8d65b8d6993c>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-ec2fc7a40586>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnumpy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbinary_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.9.6, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\n",
      "rootdir: /Users/macpro/Desktop/computer-science/projects/ai/holypics\n",
      "plugins: anyio-3.2.1, typeguard-2.13.3\n",
      "collected 0 items\n",
      "\n",
      "=============================== warnings summary ===============================\n",
      "../../../../../.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/_pytest/config/__init__.py:1114\n",
      "  /Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/_pytest/config/__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: typeguard\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "============================== 1 warning in 9.07s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.NO_TESTS_COLLECTED: 5>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "all_metrics = [\n",
    "    metrics.binary_accuracy,\n",
    "    metrics.categorical_accuracy,\n",
    "    metrics.mean_squared_error,\n",
    "    metrics.mean_absolute_error,\n",
    "    metrics.mean_absolute_percentage_error,\n",
    "    metrics.mean_squared_logarithmic_error,\n",
    "    metrics.squared_hinge,\n",
    "    metrics.hinge,\n",
    "    metrics.categorical_crossentropy,\n",
    "    metrics.binary_crossentropy,\n",
    "    metrics.poisson,\n",
    "    metrics.cosine_proximity,\n",
    "    # metrics.matthews_correlation,\n",
    "]\n",
    "\n",
    "all_sparse_metrics = [\n",
    "    metrics.sparse_categorical_accuracy,\n",
    "    metrics.sparse_categorical_crossentropy,\n",
    "]\n",
    "\n",
    "\n",
    "def test_metrics():\n",
    "    y_a = K.variable(np.random.random((6, 7)))\n",
    "    y_b = K.variable(np.random.random((6, 7)))\n",
    "    for metric in all_metrics:\n",
    "        output = metric(y_a, y_b)\n",
    "        assert K.eval(output).shape == ()\n",
    "\n",
    "\n",
    "def test_matthews_correlation():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.matthews_corrcoef\n",
    "    expected = -0.14907119849998601\n",
    "\n",
    "    actual = K.eval(metrics.matthews_correlation(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_precision():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.precision_score\n",
    "    expected = 0.40000000000000002\n",
    "\n",
    "    actual = K.eval(metrics.precision(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_recall():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.recall_score\n",
    "    expected = 0.2857142857142857\n",
    "\n",
    "    actual = K.eval(metrics.recall(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_fbeta_score():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.fbeta_score\n",
    "    expected = 0.30303030303030304\n",
    "\n",
    "    actual = K.eval(metrics.fbeta_score(y_true, y_pred, beta=2))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_fmeasure():\n",
    "    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n",
    "    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n",
    "\n",
    "    # Calculated using sklearn.metrics.f1_score\n",
    "    expected = 0.33333333333333331\n",
    "\n",
    "    actual = K.eval(metrics.fmeasure(y_true, y_pred))\n",
    "    epsilon = 1e-05\n",
    "    assert expected - epsilon <= actual <= expected + epsilon\n",
    "\n",
    "\n",
    "def test_sparse_metrics():\n",
    "    for metric in all_sparse_metrics:\n",
    "        y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n",
    "        y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n",
    "        assert K.eval(metric(y_a, y_b)).shape == ()\n",
    "\n",
    "\n",
    "def test_top_k_categorical_accuracy():\n",
    "    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n",
    "    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n",
    "    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=3))\n",
    "    assert success_result == 1\n",
    "    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=2))\n",
    "    assert partial_result == 0.5\n",
    "    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
    "                                                               k=1))\n",
    "    assert failure_result == 0\n",
    "\n",
    "\n",
    "\n",
    "pytest.main([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
