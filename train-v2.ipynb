{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32\n",
    "data_dir = \"images_new\"\n",
    "nsfw_classes_data = [{\"name\": \"general_nsfw\",\"index\": 5}]\n",
    "binary_classes_names = [\"adult\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "found 954 for class general_not_nsfw_not_suggestive\n",
      "found 760 for class female_nudity\n",
      "found 926 for class female_swimwear\n",
      "found 588 for class male_underwear_or_shirtless\n",
      "found 860 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 3266 images belonging to 5 classes.\n",
      "Found 814 images belonging to 5 classes.\n",
      "class_weights =>  {0: 0.7666340508806262, 1: 0.8140900195694716, 2: 0.7734833659491194, 3: 0.8561643835616438, 4: 0.7896281800391389}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_2 (KerasLayer)   (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 21,813,029\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "103/103 [==============================] - 123s 1s/step - loss: 1.8806 - accuracy: 0.2266 - val_loss: 1.7929 - val_accuracy: 0.2125\n",
      "Epoch 2/30\n",
      "103/103 [==============================] - 153s 1s/step - loss: 1.6403 - accuracy: 0.2875 - val_loss: 1.6416 - val_accuracy: 0.2776\n",
      "Epoch 3/30\n",
      "103/103 [==============================] - 172s 2s/step - loss: 1.5117 - accuracy: 0.3536 - val_loss: 1.5274 - val_accuracy: 0.3428\n",
      "Epoch 4/30\n",
      "103/103 [==============================] - 160s 2s/step - loss: 1.4039 - accuracy: 0.4225 - val_loss: 1.4298 - val_accuracy: 0.4079\n",
      "Epoch 5/30\n",
      "103/103 [==============================] - 321s 3s/step - loss: 1.3092 - accuracy: 0.4789 - val_loss: 1.3447 - val_accuracy: 0.4582\n",
      "Epoch 6/30\n",
      "103/103 [==============================] - 519s 5s/step - loss: 1.2269 - accuracy: 0.5288 - val_loss: 1.2686 - val_accuracy: 0.4939\n",
      "Epoch 7/30\n",
      "103/103 [==============================] - 243s 2s/step - loss: 1.1536 - accuracy: 0.5686 - val_loss: 1.2020 - val_accuracy: 0.5369\n",
      "Epoch 8/30\n",
      "103/103 [==============================] - 244s 2s/step - loss: 1.0887 - accuracy: 0.6020 - val_loss: 1.1454 - val_accuracy: 0.5602\n",
      "Epoch 9/30\n",
      "103/103 [==============================] - 226s 2s/step - loss: 1.0315 - accuracy: 0.6246 - val_loss: 1.0929 - val_accuracy: 0.5835\n",
      "Epoch 10/30\n",
      "103/103 [==============================] - 241s 2s/step - loss: 0.9823 - accuracy: 0.6503 - val_loss: 1.0491 - val_accuracy: 0.6192\n",
      "Epoch 11/30\n",
      "103/103 [==============================] - 232s 2s/step - loss: 0.9380 - accuracy: 0.6705 - val_loss: 1.0099 - val_accuracy: 0.6327\n",
      "Epoch 12/30\n",
      "103/103 [==============================] - 232s 2s/step - loss: 0.8990 - accuracy: 0.6855 - val_loss: 0.9756 - val_accuracy: 0.6462\n",
      "Epoch 13/30\n",
      "103/103 [==============================] - 311s 3s/step - loss: 0.8641 - accuracy: 0.7018 - val_loss: 0.9449 - val_accuracy: 0.6572\n",
      "Epoch 14/30\n",
      "103/103 [==============================] - 349s 3s/step - loss: 0.8325 - accuracy: 0.7119 - val_loss: 0.9178 - val_accuracy: 0.6597\n",
      "Epoch 15/30\n",
      "103/103 [==============================] - 342s 3s/step - loss: 0.8043 - accuracy: 0.7247 - val_loss: 0.8928 - val_accuracy: 0.6720\n",
      "Epoch 16/30\n",
      "103/103 [==============================] - 359s 3s/step - loss: 0.7786 - accuracy: 0.7352 - val_loss: 0.8703 - val_accuracy: 0.6781\n",
      "Epoch 17/30\n",
      "103/103 [==============================] - 220s 2s/step - loss: 0.7551 - accuracy: 0.7440 - val_loss: 0.8505 - val_accuracy: 0.6855\n",
      "Epoch 18/30\n",
      "103/103 [==============================] - 251s 2s/step - loss: 0.7341 - accuracy: 0.7511 - val_loss: 0.8314 - val_accuracy: 0.6892\n",
      "Epoch 19/30\n",
      "103/103 [==============================] - 218s 2s/step - loss: 0.7145 - accuracy: 0.7575 - val_loss: 0.8153 - val_accuracy: 0.6990\n",
      "Epoch 20/30\n",
      "103/103 [==============================] - 260s 3s/step - loss: 0.6964 - accuracy: 0.7615 - val_loss: 0.7998 - val_accuracy: 0.7027\n",
      "Epoch 21/30\n",
      "103/103 [==============================] - 267s 3s/step - loss: 0.6794 - accuracy: 0.7676 - val_loss: 0.7846 - val_accuracy: 0.7064\n",
      "Epoch 22/30\n",
      "103/103 [==============================] - 284s 3s/step - loss: 0.6639 - accuracy: 0.7697 - val_loss: 0.7724 - val_accuracy: 0.7088\n",
      "Epoch 23/30\n",
      "103/103 [==============================] - 333s 3s/step - loss: 0.6497 - accuracy: 0.7719 - val_loss: 0.7619 - val_accuracy: 0.7162\n",
      "Epoch 24/30\n",
      "103/103 [==============================] - 235s 2s/step - loss: 0.6367 - accuracy: 0.7802 - val_loss: 0.7498 - val_accuracy: 0.7162\n",
      "Epoch 25/30\n",
      "103/103 [==============================] - 216s 2s/step - loss: 0.6238 - accuracy: 0.7820 - val_loss: 0.7388 - val_accuracy: 0.7211\n",
      "Epoch 26/30\n",
      "103/103 [==============================] - 218s 2s/step - loss: 0.6122 - accuracy: 0.7863 - val_loss: 0.7298 - val_accuracy: 0.7273\n",
      "Epoch 27/30\n",
      "103/103 [==============================] - 210s 2s/step - loss: 0.6018 - accuracy: 0.7909 - val_loss: 0.7193 - val_accuracy: 0.7310\n",
      "Epoch 28/30\n",
      "103/103 [==============================] - 179s 2s/step - loss: 0.5915 - accuracy: 0.7930 - val_loss: 0.7122 - val_accuracy: 0.7359\n",
      "Epoch 29/30\n",
      "103/103 [==============================] - 158s 2s/step - loss: 0.5818 - accuracy: 0.7964 - val_loss: 0.7055 - val_accuracy: 0.7346\n",
      "Epoch 30/30\n",
      "103/103 [==============================] - 230s 2s/step - loss: 0.5729 - accuracy: 0.7991 - val_loss: 0.6965 - val_accuracy: 0.7371\n"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch = num_classes//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=3,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=30,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321ac91cb9e04ceaa272c428e58353a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515b44dd5c4846b8b232db5e7a9fb362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e72fef980d4b188a1e04c57ae1b0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/inception_v3_1642705230.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path_keras = \"models/1625175782.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Ids:            [2 3 2 1 3 2 3 2 3 4 2 3 4 1 2 3 3 3 3 2 1 0 3 0 0 0 2 3 3 2 4 3]\n",
      "Labels:            ['fr', 'ms', 'fr', 'fy', 'ms', 'fr', 'ms', 'fr', 'ms', 'gw', 'fr', 'ms', 'gw', 'fy', 'fr', 'ms', 'ms', 'ms', 'ms', 'fr', 'fy', 'ge', 'ms', 'ge', 'ge', 'ge', 'fr', 'ms', 'ms', 'fr', 'gw', 'ms']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Ids:            [4 2 1 4 3 1 4 4 0 3 1 4 3 0 2 4 2 3 2 4 2 0 3 2 2 3 1 2 3 4 2 4]\n",
      "predicted_class_names:            ['gw', 'fr', 'fy', 'gw', 'ms', 'fy', 'gw', 'gw', 'ge', 'ms', 'fy', 'gw', 'ms', 'ge', 'fr', 'gw', 'fr', 'ms', 'fr', 'gw', 'fr', 'ge', 'ms', 'fr', 'fr', 'ms', 'fy', 'fr', 'ms', 'gw', 'fr', 'gw']\n",
      "three_digit_predictions:  [0.03, 0.02, 0.61, 0.09, 0.19, 0.01, 0.03, 0.04, 0.38, 0.24, 0.0, 0.16, 0.14, 0.24, 0.03, 0.21, 0.16, 0.33, 0.02, 0.04, 0.0, 0.4, 0.13, 0.01, 0.18, 0.16, 0.0, 0.05, 0.23, 0.04, 0.16, 0.15]\n",
      "0.61\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Shapes of all inputs must match: values[0].shape = [32] != values[1].shape = [32,5] [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-f9da0942020e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mthree_digit_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthree_digit_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m cfs_matrix = tf.math.confusion_matrix(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/ops/confusion_matrix.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(labels, predictions, num_classes, weights, dtype, name)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     values = (array_ops.ones_like(predictions, dtype)\n\u001b[1;32m    194\u001b[0m               if weights is None else weights)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1421\u001b[0m                        (axis, -expanded_num_dims, expanded_num_dims))\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6398\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6399\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6400\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6401\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6402\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [32] != values[1].shape = [32,5] [Op:Pack] name: stack"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_class_names = [(lambda l, cl: cl[l][0]+cl[l][len(cl[l])-1])(label, class_names) for label in label_batch]\n",
    "three_digit_predictions = [(lambda prb: prb*100 if str(prb*100).replace(\",\", \".\").find(\".\") == -1 else int(str(prb*100).split(\".\")[0].replace(\"[\", \"\"))/100 )(prb) for prb in predicted_batch.numpy()]\n",
    "print(\"Labels Ids:           \", label_batch)\n",
    "print(\"predicted_class_names:           \",   predicted_class_names)\n",
    "print(\"three_digit_predictions: \", three_digit_predictions)\n",
    "# print(  (lambda x: x[x.index(max(x))]  )(three_digit_predictions) )\n",
    "print( three_digit_predictions[np.argmax(three_digit_predictions)] )\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31abd97d5a974f7f9425e516aaa1fb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615966851db04241a6dd352b8e3f2f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f237ebf57b43e2a5a864b7d6fe9c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  3000\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def predict_single_image_from_path(path):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(numpy.array([image_resized]))\n",
    "    class_index = 0\n",
    "    max = 0\n",
    "    for i in range(0, len(prediction[0])):\n",
    "        if prediction[0][0] > max:\n",
    "            max =  prediction[0][i]\n",
    "            class_index = i\n",
    "    print(prediction[0])\n",
    "    # class_index = int(np.argmax(prediction[0], axis=0)) #numpy.where(prediction[0]== numpy.amax(prediction[0]))[0][0]\n",
    "    return class_index, class_names[class_index], Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        class_index, class_name, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(class_name)\n",
    "        display(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        class_index, class_name, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(class_name)\n",
    "        display(image)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [5 3 3 1 2 1 2 3 3 3 2 2 2 3 1 3 2 5 1 3 0 2 2 2 2 5 1 2 1 2 2 1]\n",
      "Predicted labels:  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n",
      "precisions :  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6213079"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_prediction(predictions):\n",
    "    binary_classes_index = []\n",
    "    predictions_probs = []\n",
    "    predictions_data = [] \n",
    "    numpy_predictions = predictions.numpy()\n",
    "    binary_class_names = []\n",
    "    for prediction in numpy_predictions:\n",
    "        nsfw_pred_sum = 0\n",
    "        binary_class_index = 0\n",
    "        for nsfw_classe_data in nsfw_classes_data:\n",
    "            nsfw_pred_sum += prediction[nsfw_classe_data[\"index\"]]\n",
    "\n",
    "        nsfw_pred_prob = nsfw_pred_sum / len(nsfw_classes_data)\n",
    "        \n",
    "        binary_class_index = 0 if nsfw_pred_prob > 0.5 else 1\n",
    "        binary_classes_index.append(nsfw_pred_prob)\n",
    "        predictions_probs.append(nsfw_pred_prob)\n",
    "\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "        binary_class_names.append(binary_classes_names[binary_class_index])\n",
    "    return np.array(binary_classes_index), np.array(predictions_probs), predictions, binary_class_names, predictions_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_ids, num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-8d65b8d6993c>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-ec2fc7a40586>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnumpy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbinary_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('holypics-SxDLhKSZ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
