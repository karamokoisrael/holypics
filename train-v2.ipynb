{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {zZZ\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "    <li>Work  with imbalanced dataset => https://medium.com/geekculture/imbalanced-dataset-machine-learning-model-from-end-to-end-implementation-tensorflow-2-2-c48b5bc2eabc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements-notebook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget \"https://storage.googleapis.com/kaggle-data-sets/1885605/3083816/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220127%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220127T102630Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=173a91a8aaced400cb01f48648c997f0aac12cdbd597c4a48ae4089925004936e9e3688a71f69507c76736a13a5553f862af96fff8de4c9bc9ffea29d1ae8c2e976924e3eb329d6fd7f176b8dfb1b33cfb9fa3b000e3e916d15e55a72d9b8d8be048a808fb8fad044e59a79ad88a80fce3d8fb2fe98665dbe6be336ba8dd182f0a2b14bfa8efe46e201d94d9947d30d2f48f82925bbcaab47ab6a0b88dc7617a94704b0c84999388b312644910e369805627e3f95ce6c0f2103da2252947f46f954e20c0360c47763139ad27530fccc3d87bed953ef53005237c8a262a733446958bcf9d79196bd131c8bbfb07a8c67ce1062082221137bc336c25c0f4431fa7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv archive.zip* archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "#from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import imutils\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No compatible GPU device found\n"
     ]
    }
   ],
   "source": [
    "phisical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print(phisical_devices)\n",
    "if len(phisical_devices) > 0: \n",
    "    tf.config.experimental.set_memory_growth(phisical_devices[0], True)\n",
    "    print(\"GPU activated with {}\".format(phisical_devices[0]))\n",
    "else:\n",
    "    print(\"No compatible GPU device found\")\n",
    "# print(tf.test.is_gpu_available())\n",
    "# print(tf.config.list_pZZzhysical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "EPOCHS=30\n",
    "PATIENCE=5\n",
    "LR = 1e-4\n",
    "EARLY_STOPPING=True\n",
    "RESTORE_BEST_WEIGHT=False\n",
    "TENSORBOARD_ENABLED=True\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32#32\n",
    "data_dir = \"images_new\"\n",
    "csv_dataset = \"image_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "def interpret_prediction(predicted_batch, get_images=False, image_set=[]):\n",
    "    # np_prediction = predicted_batch.numpy()\n",
    "    decoded_predictions = []\n",
    "    decoded_main_predictions_classes = []\n",
    "    max_indices = [(lambda pr: class_names[np.argmax(pr, axis=-1)])(predicton) for predicton in predicted_batch]\n",
    "    for count in range(0, len(predicted_batch)):\n",
    "        prd_btch = predicted_batch[count]\n",
    "        decoded_part = []\n",
    "        for i in range(0, num_classes):\n",
    "            decoded_prediction = {}\n",
    "            decoded_prediction[\"class_name\"] = class_names[i]\n",
    "            try:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i].numpy()\n",
    "            except Exception as e:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i]\n",
    "            decoded_prediction[\"precision\"] = np.sum(prd_btch[i]) / num_classes\n",
    "            \n",
    "            # decoded_prediction[\"count_index\"] = count\n",
    "        \n",
    "            if get_images:\n",
    "                decoded_prediction[\"image\"] = image_set[count]\n",
    "            decoded_part.append(decoded_prediction)\n",
    "        decoded_predictions.append(decoded_part)\n",
    "        \n",
    "        decoded_main_predictions_classes.append(decoded_part)\n",
    "    return decoded_predictions, decoded_main_predictions_classes, max_indices\n",
    "    \n",
    "\n",
    "def predict_single_image_from_path(path, break_line=True):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def predict_single_raw_image(image, break_line=True):\n",
    "    prediction = model.predict(image)\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    \n",
    "    return to_print, image\n",
    "\n",
    "\n",
    "def predict_single_image_from_url(url, break_line=True):\n",
    "    image = imutils.url_to_image(url)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "         \n",
    "        try:\n",
    "            prob_str = str(prediction[0][i]*100)[0:5]\n",
    "        except Exception as wrong: \n",
    "              prob_str = str(prediction[0][i]*100)\n",
    "        str_ouput = \"{0} => {1}%; \\n\" if break_line else \"{0} => {1}%;\"\n",
    "        to_print  += str_ouput.format( class_names[i], prob_str)\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "\n",
    "def predict_from_path(path=data_dir, group=True):\n",
    "    data_dir = path\n",
    "    clean_up_data_dir()\n",
    "    images_path = []\n",
    "    \n",
    "    if(group):\n",
    "        data_sub_directories = os.listdir(data_dir)\n",
    "        for data_sub_directory in data_sub_directories:\n",
    "            # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "            print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))\n",
    "            for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "                images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "    else:\n",
    "        try:\n",
    "            for current_dir in os.listdir(data_dir):\n",
    "                images_path.append(os.path.join(data_dir, current_dir))\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "\n",
    "    if not group:\n",
    "        data_dir = \".\"\n",
    "    \n",
    "    bulk_prediction(data_dir, images_path)\n",
    "    \n",
    "def bulk_prediction(data_dir=\"\", images_path=[], images=[]):\n",
    "    current = 0\n",
    "    output = widgets.Output()\n",
    "    next_button = widgets.Button(description='Next')\n",
    "    prev_button = widgets.Button(description='Prev')\n",
    "    display_current_button = widgets.Button(description='Current')\n",
    "    current_index_text = widgets.Textarea(\n",
    "        value=str(current),\n",
    "        placeholder='current index goes here',\n",
    "        description='index',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    display(current_index_text, display_current_button, prev_button, next_button, output)\n",
    "    \n",
    "    def default_action():\n",
    "        global current\n",
    "        with output:\n",
    "            clear_output()\n",
    "            images_store = images_path if len(images_path) > 0 else images\n",
    "            \n",
    "            print(\"{0}/{1}\".format(current+1, len(images_store)))\n",
    "            if len(images_path) > 0:\n",
    "                to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "            else:\n",
    "                to_print, image = predict_single_raw_image(images[current])\n",
    "            print(to_print)\n",
    "            display(image)\n",
    "            \n",
    "    def on_next_button_clicked(_):\n",
    "        global current\n",
    "        if current+2 > len(images_path):\n",
    "            return None\n",
    "        current+=1\n",
    "        default_action()\n",
    "\n",
    "\n",
    "    def on_prev_button_clicked(_):\n",
    "        global current\n",
    "        if current-1 < 0:\n",
    "            return None\n",
    "        current-=1\n",
    "        default_action()\n",
    "        \n",
    "        \n",
    "    def on_current_index_change(_):\n",
    "        update_index_change(current_index_text.value)\n",
    "\n",
    "    def update_index_change(indexString):\n",
    "        global current\n",
    "        try:\n",
    "            current = int(indexString)\n",
    "            default_action()\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "\n",
    "    next_button.on_click(on_next_button_clicked)\n",
    "    prev_button.on_click(on_prev_button_clicked)\n",
    "    display_current_button.on_click(on_current_index_change)\n",
    "    current_index_text.on_displayed(update_index_change(str(current)))\n",
    "    \n",
    "\n",
    "def predict_at_random():    \n",
    "    base_url = \"https://picsum.photos/224/224\"\n",
    "    again_button = widgets.Button(description='Again')\n",
    "    output = widgets.Output()\n",
    "    display(again_button, output)\n",
    "\n",
    "    def on_again_button_clicked(_):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            to_print, image = predict_single_image_from_url(base_url)\n",
    "            print(to_print)\n",
    "            display(image)\n",
    "\n",
    "    again_button.on_click(on_again_button_clicked)\n",
    "    \n",
    "    \n",
    "def predict_url_batch(urls, figsize=(30, 30), verbose=False, break_line=True):\n",
    "    predictions_output = []    \n",
    "    images=[]\n",
    "    for url in urls:\n",
    "        try:\n",
    "            image = imutils.url_to_image(url)\n",
    "            # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "            imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "            image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "            images.append(np.array([image_resized]))\n",
    "        except Exception as wrong:\n",
    "            pass\n",
    "    bulk_prediction(images=images)\n",
    "    \n",
    "def predict_from_txt_file(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False, break_line=True):\n",
    "    urls = []\n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)       \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predict_url_batch(urls, figsize=figsize, verbose=verbose, break_line=break_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'images_newfemale_nudity.*': No such file or directory\n",
      "rm: cannot remove 'images_newfemale_swimwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newfemale_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_not_nsfw_not_suggestive.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_nsfw.*': No such file or directory\n",
      "rm: cannot remove 'images_newmale_shirtless.*': No such file or directory\n",
      "rm: cannot remove 'images_newmale_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_new/.ipynb_checkpoints': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1500 for class female_nudity\n",
      "found 739 for class female_swimwear\n",
      "found 1500 for class female_underwear\n",
      "found 1500 for class general_not_nsfw_not_suggestive\n",
      "found 1500 for class general_nsfw\n",
      "found 429 for class male_shirtless\n",
      "found 156 for class male_underwear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'images_new/.DS_Store': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'images_newfemale_nudity.*': No such file or directory\n",
      "rm: cannot remove 'images_newfemale_swimwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newfemale_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_not_nsfw_not_suggestive.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_nsfw.*': No such file or directory\n",
      "rm: cannot remove 'images_newmale_shirtless.*': No such file or directory\n",
      "rm: cannot remove 'images_newmale_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_new/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'images_new/.DS_Store': No such file or directory\n",
      "C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:279: UserWarning: Found 8 invalid image filename(s) in x_col=\"filenames\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5853 validated image filenames.\n",
      "Found 1463 validated image filenames.\n",
      "class_weights =>  {0: 0.795193883123976, 1: 0.8990988530857456, 2: 0.795193883123976, 3: 0.795193883123976, 4: 0.795193883123976, 5: 0.9414254505734572, 6: 0.9787001638448936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ISRAEL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py:279: UserWarning: Found 8 invalid image filename(s) in x_col=\"filenames\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(csv_dataset)\n",
    "columns=data_sub_directories\n",
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "training_set=train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=data_dir,\n",
    "    x_col=\"filenames\",\n",
    "    y_col=columns,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode=\"raw\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "validation_set=train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=data_dir,\n",
    "    x_col=\"filenames\",\n",
    "    y_col=columns,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode=\"raw\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "class_names = columns\n",
    "num_classes = len(class_names)\n",
    "num_samples = training_set.samples + validation_set.samples\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_3 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 8967      \n",
      "=================================================================\n",
      "Total params: 2,266,951\n",
      "Trainable params: 8,967\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     # layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     # tf.keras.layers.Flatten(),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(num_classes, activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=\"binary_crossentropy\",\n",
    "  metrics=[\"accuracy\"]\n",
    ")\n",
    "# model.compile(\n",
    "#   optimizer=tf.keras.optimizers.RMSprop(learning_rate=LR, decay=DC),\n",
    "#   loss=\"binary_crossentropy\",\n",
    "#   metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# model.compile\n",
    "#   optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "#   loss=macro_soft_f1,#\"categorical_crossentropy\",\n",
    "#   metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#   optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "#   loss=macro_soft_f1,\n",
    "#   metrics=[macro_f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "callbacks = [model_checkpoint_callback]\n",
    "\n",
    "if TENSORBOARD_ENABLED:\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_callback)\n",
    "\n",
    "if EARLY_STOPPING:\n",
    "    stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=PATIENCE,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    "    )\n",
    "    callbacks.append(stop_training_callback)\n",
    "\n",
    "# if TENSORBOARD_ENABLED:\n",
    "# %tensorboard --logdir logs/fit\n",
    "    \n",
    "history = model.fit(training_set,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=callbacks,\n",
    "                    # class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESTORE_BEST_WEIGHT:\n",
    "    model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['accuracy']\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mobilenet_v2_1643910921.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 10248     \n",
      "=================================================================\n",
      "Total params: 2,268,232\n",
      "Trainable params: 10,248\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "export_path_keras = \"models/mobilenet_v2_1643808443.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'images_newfemale_nudity.*': No such file or directory\n",
      "rm: cannot remove 'images_newfemale_swimwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newfemale_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_not_nsfw_not_suggestive.*': No such file or directory\n",
      "rm: cannot remove 'images_newgeneral_nsfw.*': No such file or directory\n",
      "rm: cannot remove 'images_newmale_shirtless.*': No such file or directory\n",
      "rm: cannot remove 'images_newmale_underwear.*': No such file or directory\n",
      "rm: cannot remove 'images_new/.ipynb_checkpoints': No such file or directory\n",
      "rm: cannot remove 'images_new/.DS_Store': No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cccc9bc744f47059cb08ba0c42f2088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='0', description='index', placeholder='current index goes here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15229d58dc4640d2a8ea45926772b5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Current', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacbfa0d5687427cbe0ae31336b9f842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f5cb39a72a430ea1af9a857558f34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0354ad1965c54fc096bc158e37f5395a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_path(\"images_backup/test/clothed_own\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490570594deb4d24a3f97e1ff1bcc7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Again', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef946fc05b04677905383de011660be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_at_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e1901e493748e1b0663d378b13fe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='0', description='index', placeholder='current index goes here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a805a6fed0414fb4bcc737286a6eae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Current', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff5c3621374c148f290ce4ea298045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e809bc05ae645c79eeb7954bec15ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c21123224c9403a961b757d39832a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_urls = [\n",
    "    \"https://i.ytimg.com/vi/yWI61kpFEAA/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDRAPwFXV09U5Eo-fhoUnh7FTbp1w\",\n",
    "    \"https://i.ytimg.com/vi/EiXQmeuHTOY/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBz-YYzwt-B30cjMrXYzm0PopCukg\",\n",
    "    \"https://i.ytimg.com/vi/poQXNp9ItL4/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLAyT3wtstrlzKYaC9sGf05ea66wmg\"\n",
    "]\n",
    "predict_url_batch(current_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_from_txt_file(src='validation-adult-save.txt', start=30, limit=40, break_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
