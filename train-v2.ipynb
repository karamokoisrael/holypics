{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 data \n",
    "\n",
    "  {\n",
    "    \"time\": 0,\n",
    "    \"classes\": [\n",
    "      {\n",
    "        \"class\": \"general_not_nsfw_not_suggestive\",\n",
    "        \"score\": 0.9993004548947556\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_nsfw\",\n",
    "        \"score\": 0.00005515861332392431\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"general_suggestive\",\n",
    "        \"score\": 0.0006443864919204179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_underwear\",\n",
    "        \"score\": 0.899250297625593\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_underwear\",\n",
    "        \"score\": 0.10074970237440699\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_underwear\",\n",
    "        \"score\": 0.9961647811377407\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_underwear\",\n",
    "        \"score\": 0.0038352188622594527\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_sex_toy\",\n",
    "        \"score\": 0.9999999798312891\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_sex_toy\",\n",
    "        \"score\": 2.0168710930836975e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_nudity\",\n",
    "        \"score\": 0.7622752597582456\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_nudity\",\n",
    "        \"score\": 0.23772474024175438\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_nudity\",\n",
    "        \"score\": 0.9706443527545361\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_nudity\",\n",
    "        \"score\": 0.029355647245463922\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_female_swimwear\",\n",
    "        \"score\": 0.999611244248107\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_female_swimwear\",\n",
    "        \"score\": 0.0003887557518931324\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_male_shirtless\",\n",
    "        \"score\": 0.6499119967458475\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_male_shirtless\",\n",
    "        \"score\": 0.35008800325415235\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_text\",\n",
    "        \"score\": 0.45322065582766496\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"text\",\n",
    "        \"score\": 0.5467793441723351\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated\",\n",
    "        \"score\": 0.11259401438317206\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"hybrid\",\n",
    "        \"score\": 0.030002950239859178\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"natural\",\n",
    "        \"score\": 0.8574030353769687\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"animated_gun\",\n",
    "        \"score\": 1.2162167936901165e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_in_hand\",\n",
    "        \"score\": 0.004522403985289621\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"gun_not_in_hand\",\n",
    "        \"score\": 0.00023331984987421487\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_gun\",\n",
    "        \"score\": 0.9952442749486193\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"culinary_knife_in_hand\",\n",
    "        \"score\": 5.932730985401978e-9\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_in_hand\",\n",
    "        \"score\": 0.0018882816682760986\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"knife_not_in_hand\",\n",
    "        \"score\": 0.003480484685850096\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_knife\",\n",
    "        \"score\": 0.9946312277131428\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"a_little_bloody\",\n",
    "        \"score\": 0.00020642045767688616\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_blood\",\n",
    "        \"score\": 0.9997831147054382\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"other_blood\",\n",
    "        \"score\": 9.653595868250288e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"very_bloody\",\n",
    "        \"score\": 0.00000949947729795773\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_pills\",\n",
    "        \"score\": 0.9999999868927427\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_pills\",\n",
    "        \"score\": 1.3107257304315686e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_smoking\",\n",
    "        \"score\": 0.9999888406757149\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_smoking\",\n",
    "        \"score\": 0.000011159324285029952\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"illicit_injectables\",\n",
    "        \"score\": 0.0014406553701263015\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"medical_injectables\",\n",
    "        \"score\": 3.68515180826588e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_injectables\",\n",
    "        \"score\": 0.9985593077783557\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_nazi\",\n",
    "        \"score\": 0.9999999899241184\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_nazi\",\n",
    "        \"score\": 1.0075881556615458e-8\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_kkk\",\n",
    "        \"score\": 0.9999900152198961\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_kkk\",\n",
    "        \"score\": 0.000009984780103926167\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_middle_finger\",\n",
    "        \"score\": 0.9999998928595047\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_middle_finger\",\n",
    "        \"score\": 1.0714049516372813e-7\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"no_terrorist\",\n",
    "        \"score\": 0.9999998805523179\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"yes_terrorist\",\n",
    "        \"score\": 1.1944768206346446e-7\n",
    "      }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Use of f1_score, real => https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/macpro/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image \n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from IPython.display import Image as IImage \n",
    "import ipywidgets as widgets\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RES = 224\n",
    "dimensions = (IMAGE_RES, IMAGE_RES)\n",
    "batch_size = 32\n",
    "data_dir = \"images_new\"\n",
    "nsfw_classes_data = [{\"name\": \"general_nsfw\",\"index\": 5}]\n",
    "binary_classes_names = [\"adult\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "            \n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # compute the dimensions of the next image in the pyramid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "def sub_plot_images(image, title,elem_place=1,show = True, figsize=(1, 1), plt_hspace = 0.8, vertical=1, horizontal=5):\n",
    "    if show:\n",
    "        if not figsize == (1, 1):\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.subplot(vertical,horizontal,elem_place)\n",
    "        plt.subplots_adjust(hspace = plt_hspace)\n",
    "        plt.title(title)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        \n",
    "def detect_adult_picture_from_url(url, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    req = requests.get(url, stream=True)\n",
    "    image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "    imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "    detect_adult_picture(imageRGB, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "    \"\"\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    image_loaded = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "    detect_adult_picture(image_loaded/255, prod, plotprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "def predict_from_file_url(count_start=0, count_set = 10, src=\"validation-adult.txt\"):\n",
    "    figsize = (40, 40)\n",
    "    image_input_file = open(src, \"r\")\n",
    "    image_input_file = [image_input_fileS for image_input_fileS in image_input_file]\n",
    "    total = len(image_input_file)\n",
    "    \n",
    "    for url in image_input_file[count_start:count_set]:\n",
    "        try:\n",
    "            detect_adult_picture_from_url(url, True, False)\n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "def detect_adult_picture_from_array(array, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    detect_adult_picture(array, prod, pass_neutral, figsize, WIDTH, PYR_SCALE, WIN_STEP, ROI_SIZE, INPUT_SIZE, probaLimit)\n",
    "\n",
    "\n",
    "def calculate_average(pred):\n",
    "    if pred == 0:\n",
    "        return 1\n",
    "    elif pred < 0.5 and pred !=0:\n",
    "        return (0.5-pred)/0.5\n",
    "    elif pred >= 0.5 and pred !=1:\n",
    "         return (pred-0.5)/0.5\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def decode_prediction(predictions):\n",
    "    decoded_class_index = []\n",
    "    decode_prediction_precision = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        result = 0 if prediction < 0.5 else 1\n",
    "        precision = calculate_average(prediction)\n",
    "        decoded_class_index.append(result)\n",
    "        decode_prediction_precision.append(precision)\n",
    "    return np.array(decoded_class_index), np.array(decode_prediction_precision),predictions\n",
    "\n",
    "\n",
    "def detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.5):\n",
    "    plt.figure(figsize=figsize)\n",
    "    orig = image\n",
    "    scanned = orig.copy()\n",
    "    neutral = scanned\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    sub_plot_images(orig, \"input\", 1, prod)\n",
    "\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    count = 0\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(np.argmax(preds[count], axis=-1))]\n",
    "        prob = 1\n",
    "        if prob >= probaLimit:\n",
    "            box = locs[i]\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "        count+=1\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(scanned, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(scanned, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        # show the output after apply non-maxima suppression\n",
    "        \n",
    "    sub_plot_images(scanned, \"scanned\", 2, prod)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    sub_plot_images(clone, \"output\", 3, prod)\n",
    "    \n",
    "    \n",
    "def detect_adult_picture_no_plot(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224), probaLimit = 0.8, ksize = (51,51)):\n",
    "    \n",
    "    main_ids, main_probs, main_preds =  decode_prediction(model.predict(np.array([cv2.resize(image, INPUT_SIZE)])))\n",
    "    if main_probs[0] > probaLimit :\n",
    "        return cv2.blur(image, ksize) \n",
    "    \n",
    "    orig = image\n",
    "    copy = orig.copy()\n",
    "    orig = imutils.resize(orig, width=WIDTH)\n",
    "    \n",
    "    (H, W) = orig.shape[:2]\n",
    "    pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "    # initialize two lists, one to hold the ROIs generated from the image\n",
    "    # pyramid and sliding window, and another list used to store the\n",
    "    # (x, y)-coordinates of where the ROI was in the original image\n",
    "    rois = []\n",
    "    locs = []\n",
    "    # time how long it takes to loop over the image pyramid layers and\n",
    "    # sliding window locations\n",
    "    start = time.time()\n",
    "    for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "        # for each layer of the image pyramid, loop over the sliding\n",
    "        # window locations\n",
    "        for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "            # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "            # *original* image dimensions\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "            # take the ROI and preprocess it so we can later classify\n",
    "            # the region using Keras/TensorFlow\n",
    "            roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            # update our list of ROIs and associated coordinates\n",
    "            rois.append(roi)\n",
    "            locs.append((x, y, x + w, y + h))\n",
    "    end = time.time()\n",
    "    print(\"[INFO] looping over pyramid/windows took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # convert the ROIs to a NumPy array\n",
    "    rois = np.array(rois, dtype=\"float32\")\n",
    "    # classify each of the proposal ROIs using ResNet and then show how\n",
    "    # long the classifications took\n",
    "    print(\"[INFO] classifying ROIs...\")\n",
    "    start = time.time()\n",
    "    preds = model.predict(rois)\n",
    "    end = time.time()\n",
    "    print(\"[INFO] classifying ROIs took {:.5f} seconds\".format(\n",
    "        end - start))\n",
    "    # decode the predictions and initialize a dictionary which maps class\n",
    "    # labels (keys) to any ROIs associated with that label (values)\n",
    "    #preds = tf.keras.applications.mobilenet_v2.decode_predictions(preds, top=5)\n",
    "    labels = {}\n",
    "    tot = len(preds)\n",
    "    probaLimit = 0.5\n",
    "\n",
    "    for i in range(0, tot):\n",
    "        label = class_names[int(preds[i])]\n",
    "        prob = 1\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "    for label in labels.keys():\n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = orig.copy()\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "        # show the results *before* applying non-maxima suppression, then\n",
    "        # clone the image again so we can display the results *after*\n",
    "        # applying non-maxima suppression\n",
    "        #plt.imshow(clone)\n",
    "        clone = orig.copy()\n",
    "    # extract the bounding boxes and associated prediction\n",
    "    # probabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    \n",
    "    \n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        if label==\"neutral\":\n",
    "            pass\n",
    "        else:\n",
    "            topLeft =  (startX, startY)\n",
    "            bottomRight = (endX, endY)\n",
    "            x, y = topLeft[0], topLeft[1]\n",
    "            w, h = bottomRight[0] - topLeft[0], bottomRight[1] - topLeft[1]\n",
    "\n",
    "            # Grab ROI with Numpy slicing and blur\n",
    "            ROI = clone[y:y+h, x:x+w]\n",
    "            blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "            clone[y:y+h, x:x+w] = blur\n",
    "            \n",
    "    return clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_batch(images):\n",
    "    predicted_indexes, confidences, predictions = decode_prediction(model.predict(np.array(images)))\n",
    "    predicted_labels = []\n",
    "    for predicted_index in predicted_indexes:\n",
    "        #print(predictions[i])\n",
    "        predicted_labels.append(class_names[predicted_index])\n",
    "        \n",
    "    return predicted_labels, confidences, predicted_indexes\n",
    "\n",
    "\n",
    "def predict_from_txt_urls(src='test-urls.txt', start=0, limit=10, figsize=(30, 30), verbose=False):\n",
    "    urls = []\n",
    "    \n",
    "    with open(src) as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        tot = len(lines)\n",
    "        count = 0\n",
    "        for url in lines[start:limit]:\n",
    "            count+=1\n",
    "            urls.append(url)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                \n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "\n",
    "    predict_from_urls(urls, figsize=figsize, verbose=verbose)\n",
    "        \n",
    "        \n",
    "def predict_from_urls(urls, figsize=(30, 30), verbose=False):\n",
    "    images = []\n",
    "    tot = len(urls)\n",
    "    count=0\n",
    "    for url in urls:\n",
    "            count+=1\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"dwd => \", url)\n",
    "                req = requests.get(url, stream=True)\n",
    "                image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "                imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "                imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                images.append(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255)\n",
    "            except Exception as wrong:\n",
    "                if verbose:\n",
    "                    print(count, \"/\", tot, \"error => \",wrong)\n",
    "                pass\n",
    "    predicted_labels, confidences, predicted_indexes = predict_batch(np.array(images))\n",
    "    \n",
    "    rangeTot = len(images)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        plt.title(predicted_labels[0]+\" \"+str(confidences[0]))\n",
    "        plt.imshow(images[0])\n",
    "    else:  \n",
    "        for i in range(rangeTot):\n",
    "            plt.subplot(rangeTot,int((rangeTot)/2),i+1)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            #color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "            plt.title(predicted_labels[i]+\" \"+str(confidences[i]))#, color=color)\n",
    "            #plt.imshow(images[i]/255 if predicted_labels[i]==\"neutral\" else ndimage.gaussian_filter(images[i]/255, sigma=2))\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "def clean_up_data_dir():\n",
    "    data_sub_directories = os.listdir(data_dir)\n",
    "    for data_sub_directory in data_sub_directories:\n",
    "        path_to_delete = os.path.join(data_dir, data_sub_directory, \".*\")\n",
    "        !rm -r $path_to_delete\n",
    "\n",
    "    !rm -r $data_dir/.ipynb_checkpoints\n",
    "    !rm -r $data_dir/.DS_Store\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost\n",
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which wse predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/.DS_Store/.*\n",
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "found 954 for class general_not_nsfw_not_suggestive\n",
      "found 760 for class female_nudity\n",
      "found 909 for class female_swimwear\n",
      "found 588 for class male_underwear_or_shirtless\n",
      "found 860 for class general_nsfw\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    print(\"found {0} for class {1}\".format(len(os.listdir(os.path.join(data_dir, data_sub_directory))), data_sub_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n",
      "Found 3266 images belonging to 5 classes.\n",
      "Found 814 images belonging to 5 classes.\n",
      "class_weights =>  {0: 0.7666340508806262, 1: 0.8140900195694716, 2: 0.7734833659491194, 3: 0.8561643835616438, 4: 0.7896281800391389}\n"
     ]
    }
   ],
   "source": [
    "clean_up_data_dir()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #rotation_range=10,\n",
    "    #brightness_range=[0.2,1.2],\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.4,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    data_dir, # same directory as training data\n",
    "    target_size=dimensions,\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='categorical',\n",
    "    class_mode='sparse',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "class_names = list(training_set.class_indices)\n",
    "num_classes = len(class_names)\n",
    "files_per_class = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(data_dir + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "print (\"class_weights => \", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMPORT BASE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "# URL = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "try:\n",
    "    MODEL_BASE_NAME = URL.split(\"/\")[5]+\"_\"\n",
    "except Exception as e:\n",
    "    MODEL_BASE_NAME=\"model_\"\n",
    "feature_extractor = hub.KerasLayer(URL,\n",
    "                                   input_shape=(IMAGE_RES, IMAGE_RES, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_extractor,\n",
    "#     layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "#     layers.Dense(num_classes, activation='sigmoid', name='output')\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "103/103 [==============================] - 70s 653ms/step - loss: 1.9417 - accuracy: 0.2128 - val_loss: 1.8263 - val_accuracy: 0.2457\n",
      "Epoch 2/30\n",
      "103/103 [==============================] - 66s 639ms/step - loss: 1.7708 - accuracy: 0.2673 - val_loss: 1.6926 - val_accuracy: 0.3034\n",
      "Epoch 3/30\n",
      "103/103 [==============================] - 75s 732ms/step - loss: 1.6527 - accuracy: 0.3169 - val_loss: 1.5929 - val_accuracy: 0.3428\n",
      "Epoch 4/30\n",
      "103/103 [==============================] - 78s 757ms/step - loss: 1.5593 - accuracy: 0.3628 - val_loss: 1.5094 - val_accuracy: 0.3784\n",
      "Epoch 5/30\n",
      "103/103 [==============================] - 82s 794ms/step - loss: 1.4793 - accuracy: 0.3996 - val_loss: 1.4371 - val_accuracy: 0.4165\n",
      "Epoch 6/30\n",
      "103/103 [==============================] - 83s 808ms/step - loss: 1.4078 - accuracy: 0.4302 - val_loss: 1.3715 - val_accuracy: 0.4459\n",
      "Epoch 7/30\n",
      "103/103 [==============================] - 73s 706ms/step - loss: 1.3419 - accuracy: 0.4636 - val_loss: 1.3112 - val_accuracy: 0.4840\n",
      "Epoch 8/30\n",
      "103/103 [==============================] - 75s 731ms/step - loss: 1.2808 - accuracy: 0.4976 - val_loss: 1.2560 - val_accuracy: 0.5209\n",
      "Epoch 9/30\n",
      "103/103 [==============================] - 74s 723ms/step - loss: 1.2250 - accuracy: 0.5254 - val_loss: 1.2048 - val_accuracy: 0.5541\n",
      "Epoch 10/30\n",
      "103/103 [==============================] - 74s 723ms/step - loss: 1.1727 - accuracy: 0.5536 - val_loss: 1.1569 - val_accuracy: 0.5835\n",
      "Epoch 11/30\n",
      "103/103 [==============================] - 78s 756ms/step - loss: 1.1240 - accuracy: 0.5759 - val_loss: 1.1124 - val_accuracy: 0.5971\n",
      "Epoch 12/30\n",
      "103/103 [==============================] - 78s 756ms/step - loss: 1.0786 - accuracy: 0.6007 - val_loss: 1.0714 - val_accuracy: 0.6204\n",
      "Epoch 13/30\n",
      "103/103 [==============================] - 75s 725ms/step - loss: 1.0364 - accuracy: 0.6255 - val_loss: 1.0327 - val_accuracy: 0.6425\n",
      "Epoch 14/30\n",
      "103/103 [==============================] - 77s 748ms/step - loss: 0.9971 - accuracy: 0.6433 - val_loss: 0.9974 - val_accuracy: 0.6523\n",
      "Epoch 15/30\n",
      "103/103 [==============================] - 77s 750ms/step - loss: 0.9603 - accuracy: 0.6617 - val_loss: 0.9650 - val_accuracy: 0.6671\n",
      "Epoch 16/30\n",
      "103/103 [==============================] - 77s 751ms/step - loss: 0.9264 - accuracy: 0.6816 - val_loss: 0.9347 - val_accuracy: 0.6867\n",
      "Epoch 17/30\n",
      "103/103 [==============================] - 79s 770ms/step - loss: 0.8949 - accuracy: 0.6960 - val_loss: 0.9064 - val_accuracy: 0.6966\n",
      "Epoch 18/30\n",
      "103/103 [==============================] - 80s 773ms/step - loss: 0.8656 - accuracy: 0.7103 - val_loss: 0.8803 - val_accuracy: 0.7101\n",
      "Epoch 19/30\n",
      "103/103 [==============================] - 78s 762ms/step - loss: 0.8385 - accuracy: 0.7186 - val_loss: 0.8567 - val_accuracy: 0.7199\n",
      "Epoch 20/30\n",
      "103/103 [==============================] - 77s 751ms/step - loss: 0.8133 - accuracy: 0.7284 - val_loss: 0.8349 - val_accuracy: 0.7224\n",
      "Epoch 21/30\n",
      "103/103 [==============================] - 79s 765ms/step - loss: 0.7900 - accuracy: 0.7373 - val_loss: 0.8147 - val_accuracy: 0.7359\n",
      "Epoch 22/30\n",
      "103/103 [==============================] - 91s 878ms/step - loss: 0.7680 - accuracy: 0.7459 - val_loss: 0.7955 - val_accuracy: 0.7457\n",
      "Epoch 23/30\n",
      "103/103 [==============================] - 79s 763ms/step - loss: 0.7475 - accuracy: 0.7538 - val_loss: 0.7775 - val_accuracy: 0.7494\n",
      "Epoch 24/30\n",
      "103/103 [==============================] - 81s 785ms/step - loss: 0.7280 - accuracy: 0.7615 - val_loss: 0.7608 - val_accuracy: 0.7518\n",
      "Epoch 25/30\n",
      "103/103 [==============================] - 81s 789ms/step - loss: 0.7100 - accuracy: 0.7691 - val_loss: 0.7452 - val_accuracy: 0.7543\n",
      "Epoch 26/30\n",
      "103/103 [==============================] - 85s 826ms/step - loss: 0.6930 - accuracy: 0.7728 - val_loss: 0.7303 - val_accuracy: 0.7617\n",
      "Epoch 27/30\n",
      "103/103 [==============================] - 81s 782ms/step - loss: 0.6769 - accuracy: 0.7789 - val_loss: 0.7164 - val_accuracy: 0.7629\n",
      "Epoch 28/30\n",
      "103/103 [==============================] - 80s 775ms/step - loss: 0.6617 - accuracy: 0.7844 - val_loss: 0.7036 - val_accuracy: 0.7678\n",
      "Epoch 29/30\n",
      "103/103 [==============================] - 83s 803ms/step - loss: 0.6475 - accuracy: 0.7890 - val_loss: 0.6916 - val_accuracy: 0.7666\n",
      "Epoch 30/30\n",
      "103/103 [==============================] - 83s 802ms/step - loss: 0.6340 - accuracy: 0.7942 - val_loss: 0.6796 - val_accuracy: 0.7740\n"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch = num_classes//batch_size\n",
    "checkpoint_filepath = 'models/epoch/chk.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "stop_training_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "\n",
    "    #min_delta=0,\n",
    "    patience=3,\n",
    "    #verbose=0,\n",
    "    #mode=\"auto\",\n",
    "    #baseline=None,\n",
    "    #restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(training_set,\n",
    "                    epochs=30,\n",
    "                    # steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_set,\n",
    "                    callbacks=[model_checkpoint_callback, stop_training_callback],\n",
    "                    # class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.epoch, acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, loss, label='Training Loss')\n",
    "plt.plot(history.epoch, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f276e993ab674573a8e7bc55be489cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506e5080e8f14e828dd9cd6ccbafd20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d847bee7de1482e8e7bee0aaad237c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  0\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        pil_img = IImage(filename=os.path.join(data_dir, images_path[current]), width = dimensions[0], height=dimensions[1])\n",
    "        display(pil_img)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/mobilenet_v2_1642719832.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "export_path_keras = \"models/{0}{1}.h5\".format(MODEL_BASE_NAME, int(t))\n",
    "print(export_path_keras)\n",
    "\n",
    "model.save(export_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteDisconnected",
     "evalue": "Remote end closed connection without response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c513eb34c78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1624998901\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#export_path_keras = \"models/first-good-model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = tf.keras.models.load_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mexport_path_keras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# `custom_objects` tells keras how to load a `hub.KerasLayer`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         if (h5py is not None and\n\u001b[1;32m    200\u001b[0m             (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 201\u001b[0;31m           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0m\u001b[1;32m    202\u001b[0m                                                   compile)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    181\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m       layer = layer_module.deserialize(layer_config,\n\u001b[0m\u001b[1;32m    498\u001b[0m                                        custom_objects=custom_objects)\n\u001b[1;32m    499\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[1;32m    158\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m           \u001b[0mdeserialized_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m       \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \"\"\"\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0m\u001b[1;32m     68\u001b[0m                                     self._lock_file_timeout_sec())\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading TF-Hub Module '%s'.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mdownload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;31m# Write module descriptor to capture information about which module was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;31m# downloaded by whom and when. The file stored at the same level as a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m       request = urllib.request.Request(\n\u001b[1;32m     62\u001b[0m           self._append_compressed_format_query(handle))\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       return resolver.DownloadManager(handle).download_and_uncompress(\n\u001b[1;32m     65\u001b[0m           response, tmp_dir)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/holypics-SxDLhKSZ/lib/python3.9/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m_call_urlopen\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Overriding this method allows setting SSL context in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    535\u001b[0m                                   '_open', req)\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1390\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# Presumably, the server closed the connection before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    286\u001b[0m                                      \" response\")\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response"
     ]
    }
   ],
   "source": [
    "export_path_keras = \"models/inception_v3_1642705230.h5\"\n",
    "#1624998901\n",
    "#export_path_keras = \"models/first-good-model.h5\"\n",
    "model = tf.keras.models.load_model(\n",
    "  export_path_keras, \n",
    "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
    "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Ids:            [3 4 3 2 0 3 4 3 3 3 3 2 4 4 1 2 0 4 1 2 3 0 4 4 4 3 0 3 1 4 1 2]\n",
      "predicted_class_names:            ['ms', 'gw', 'ms', 'fr', 'ge', 'ms', 'gw', 'ms', 'ms', 'ms', 'ms', 'fr', 'gw', 'gw', 'fy', 'fr', 'ge', 'gw', 'fy', 'fr', 'ms', 'ge', 'gw', 'gw', 'gw', 'ms', 'ge', 'ms', 'fy', 'gw', 'fy', 'fr']\n",
      "three_digit_predictions:  [0.35, 0.09, 0.34, 0.01, 0.76, 0.51, 0.46, 0.38, 0.12, 0.16, 0.58, 0.06, 0.1, 0.13, 0.07, 0.0, 0.16, 0.12, 0.1, 0.11, 0.37, 0.52, 0.08, 0.05, 0.16, 0.44, 0.53, 0.29, 0.01, 0.08, 0.09, 0.05]\n",
      "0.76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_class_names = [(lambda l, cl: cl[l][0]+cl[l][len(cl[l])-1])(label, class_names) for label in label_batch]\n",
    "three_digit_predictions = [(lambda prb: prb*100 if str(prb*100).replace(\",\", \".\").find(\".\") == -1 else int(str(prb*100).split(\".\")[0].replace(\"[\", \"\"))/100 )(prb) for prb in predicted_batch.numpy()]\n",
    "print(\"Labels Ids:           \", label_batch)\n",
    "print(\"predicted_class_names:           \",   predicted_class_names)\n",
    "print(\"three_digit_predictions: \", three_digit_predictions)\n",
    "# print(  (lambda x: x[x.index(max(x))]  )(three_digit_predictions) )\n",
    "print( three_digit_predictions[np.argmax(three_digit_predictions)] )\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_batch.numpy(), num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_prediction(predicted_batch, get_images=False, image_set=[]):\n",
    "    # np_prediction = predicted_batch.numpy()\n",
    "    decoded_predictions = []\n",
    "    decoded_main_predictions_classes = []\n",
    "    max_indices = [(lambda pr: class_names[np.argmax(pr, axis=-1)])(predicton) for predicton in predicted_batch]\n",
    "    for count in range(0, len(predicted_batch)):\n",
    "        prd_btch = predicted_batch[count]\n",
    "        decoded_part = []\n",
    "        for i in range(0, num_classes):\n",
    "            decoded_prediction = {}\n",
    "            decoded_prediction[\"class_name\"] = class_names[i]\n",
    "            try:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i].numpy()\n",
    "            except Exception as e:\n",
    "                decoded_prediction[\"probability\"] = prd_btch[i]\n",
    "            decoded_prediction[\"precision\"] = np.sum(prd_btch[i]) / num_classes\n",
    "            \n",
    "            # decoded_prediction[\"count_index\"] = count\n",
    "        \n",
    "            if get_images:\n",
    "                decoded_prediction[\"image\"] = image_set[count]\n",
    "            decoded_part.append(decoded_prediction)\n",
    "        decoded_predictions.append(decoded_part)\n",
    "        \n",
    "        decoded_main_predictions_classes.append(decoded_part)\n",
    "    return decoded_predictions, decoded_main_predictions_classes, max_indices\n",
    "    \n",
    "\n",
    "decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(predicted_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: images_new/general_not_nsfw_not_suggestive/.*\n",
      "zsh:1: no matches found: images_new/female_nudity/.*\n",
      "zsh:1: no matches found: images_new/female_swimwear/.*\n",
      "zsh:1: no matches found: images_new/male_underwear_or_shirtless/.*\n",
      "zsh:1: no matches found: images_new/general_nsfw/.*\n",
      "rm: images_new/.ipynb_checkpoints: No such file or directory\n",
      "rm: images_new/.DS_Store: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee370d1e85b40e29d5e127b690c310f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Prev', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5327175fc0f45d28eb1a2cdf7180508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170974cebef3496ea61b508db2acb173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "current  =  4000\n",
    "clean_up_data_dir()\n",
    "images_path = []\n",
    "data_sub_directories = os.listdir(data_dir)\n",
    "for data_sub_directory in data_sub_directories:\n",
    "    # images_path+=os.listdir(os.path.join(data_dir, data_sub_directory))\n",
    "    for current_dir in os.listdir(os.path.join(data_dir, data_sub_directory)):\n",
    "        images_path.append(os.path.join(data_sub_directory, current_dir))\n",
    "\n",
    "next_button = widgets.Button(description='Next')\n",
    "prev_button = widgets.Button(description='Prev')\n",
    "class_names = os.listdir(data_dir)\n",
    "moving_paths = []\n",
    "output = widgets.Output()\n",
    "display(prev_button, next_button, output)\n",
    "\n",
    "def predict_single_image_from_path(path):\n",
    "    image = cv2.imread(path)\n",
    "    # imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    imageRGB = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA)/255\n",
    "    prediction = model.predict(np.array([image_resized]))\n",
    "    decoded_predictions, decoded_main_predictions_classes, max_indices = interpret_prediction(prediction)\n",
    "\n",
    "    # to_print = \"{0} \\n {1} \\n {2}\".format(decoded_predictions, decoded_main_predictions_classes, max_indices )\n",
    "    to_print = \"\"\n",
    "    for i in range(0, len(class_names)):\n",
    "        to_print  += \"{0} => {1} \\n \".format( class_names[i],  prediction[0][i] )\n",
    "    # to_print = \"{0} \\n {1}\".format( class_names,  prediction )\n",
    "    return to_print, Image.fromarray(cv2.resize(imageRGB, dimensions, interpolation = cv2.INTER_AREA))\n",
    "\n",
    "def on_next_button_clicked(_):\n",
    "    global current\n",
    "    if current+2 > len(images_path):\n",
    "        return None\n",
    "    with output:\n",
    "        current+=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "\n",
    "\n",
    "def on_prev_button_clicked(_):\n",
    "    global current\n",
    "    if current-1 < 0:\n",
    "        return None\n",
    "    with output:\n",
    "        current-=1\n",
    "        clear_output()\n",
    "        print(\"{0}/{1}\".format(current+1, len(images_path)))\n",
    "        to_print, image = predict_single_image_from_path(os.path.join(data_dir, images_path[current]))\n",
    "        print(to_print)\n",
    "        display(image)\n",
    "\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "prev_button.on_click(on_prev_button_clicked)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:            [5 3 3 1 2 1 2 3 3 3 2 2 2 3 1 3 2 5 1 3 0 2 2 2 2 5 1 2 1 2 2 1]\n",
      "Predicted labels:  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n",
      "precisions :  [7.32293606e-01 1.32637681e-04 3.01482575e-03 1.02465638e-05\n",
      " 7.85846737e-07 1.38290910e-04 1.52818247e-04 1.39018130e-06\n",
      " 3.48762761e-08 2.71023042e-03 1.08929398e-05 8.67872245e-07\n",
      " 4.81887355e-07 5.77186574e-06 1.08480390e-05 8.87377374e-03\n",
      " 4.67095418e-07 8.73120129e-01 2.41738133e-04 6.26934506e-03\n",
      " 5.39959222e-02 1.92512180e-05 1.65759729e-07 1.58568087e-04\n",
      " 1.76813206e-04 9.95539725e-01 7.98056280e-05 1.03342484e-06\n",
      " 8.01837268e-06 1.47547587e-04 1.10551433e-07 2.06580095e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6213079"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_prediction(predictions):\n",
    "    binary_classes_index = []\n",
    "    predictions_probs = []\n",
    "    predictions_data = [] \n",
    "    numpy_predictions = predictions.numpy()\n",
    "    binary_class_names = []\n",
    "    for prediction in numpy_predictions:\n",
    "        nsfw_pred_sum = 0\n",
    "        binary_class_index = 0\n",
    "        for nsfw_classe_data in nsfw_classes_data:\n",
    "            nsfw_pred_sum += prediction[nsfw_classe_data[\"index\"]]\n",
    "\n",
    "        nsfw_pred_prob = nsfw_pred_sum / len(nsfw_classes_data)\n",
    "        \n",
    "        binary_class_index = 0 if nsfw_pred_prob > 0.5 else 1\n",
    "        binary_classes_index.append(nsfw_pred_prob)\n",
    "        predictions_probs.append(nsfw_pred_prob)\n",
    "\n",
    "        prediction_data= {}\n",
    "        for i in range(0, len(prediction)):\n",
    "            prediction_data[class_names[i]] = prediction[i]\n",
    "            predictions_data.append(prediction_data)\n",
    "        binary_class_names.append(binary_classes_names[binary_class_index])\n",
    "    return np.array(binary_classes_index), np.array(predictions_probs), predictions, binary_class_names, predictions_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds, predicted_class_names, predictions_data = decode_prediction(predicted_batch)\n",
    "\n",
    "    \n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)\n",
    "\n",
    "cfs_matrix = tf.math.confusion_matrix(\n",
    "    label_batch, predicted_ids, num_classes=num_classes\n",
    ")\n",
    "\n",
    "plt.imshow(cfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model for embeded devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "from datetime import datetime\n",
    "output_path = 'models/embeded/{}'.format(datetime.now())\n",
    "!mkdir $output_path\n",
    "tfjs.converters.save_keras_model(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/holypics/\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"shared/models/holypics/\"+str(version)\n",
    "#!rm -r $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_img_bytes(img):\n",
    "    img = tf.strings.regex_replace(img, \"\\+\", \"-\")\n",
    "    img = tf.strings.regex_replace(img, \"\\/\", \"_\")\n",
    "    image = tf.image.decode_jpeg(tf.io.decode_base64(img), channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
    "    image = tf.image.resize(images=image, size=dimensions)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        \n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "class ExportModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(self)       \n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string, name=\"base64\")\n",
    "    ])\n",
    "    \n",
    "    def serving_fn(self, base64):\n",
    "        #a = np.array([x.lower() if isinstance(x, str) else x for x in arr])\n",
    "        base64_image = tf.map_fn(lambda x: decode_img_bytes(x), base64, fn_output_signature=tf.float32)\n",
    "        preds = self.model(base64_image)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            print(sess.run(preds))\n",
    "\n",
    "        return {\n",
    "            #'base_64': base64,\n",
    "            'prediction': preds\n",
    "            #'precisions': prediction_precision\n",
    "        }\n",
    "\n",
    "    def save(self, export_path):\n",
    "        sigs = {\n",
    "            'serving_default' : self.serving_fn\n",
    "        }\n",
    "        \n",
    "        #tf.keras.backend.set_learning_phase(0) # inference only\n",
    "        tf.saved_model.save(self, export_path, signatures=sigs)\n",
    "sm = ExportModel(model)\n",
    "sm.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send deployement files to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"http://ml.megamaxdevelopment.tech/uploader.php\"\n",
    "\n",
    "payload = {'key': \"tfdmhdsus\", 'path': 'ml.megamaxdevelopment.tech/holypics/'}\n",
    "\n",
    "file = 'models/shared/shared.zip'#'models/shared/shared.zip'\n",
    "\n",
    "files = {'uploaded_file': (os.path.basename(file), open(file, 'rb'), 'application/octet-stream')}\n",
    "\n",
    "r = requests.post(url, files=files, data=payload)\n",
    "\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last deployement instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>sudo sh deploy.sh version (host)</li>\n",
    "    <li>docker-compose up (host)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview model performances on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def get_image_from_video(path= \"assets/normal-1.mp4\", start_frame = -1, sequences_number = 50):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    count = 0\n",
    "    image = np.asarray([]);\n",
    "    try:\n",
    "        while True:\n",
    "            if start_frame!=-1 and count < start_frame:\n",
    "                count+=1\n",
    "                pass\n",
    "            else:\n",
    "                ret, frame = cap.read()\n",
    "                height, width, _ = frame.shape\n",
    "\n",
    "                # Extract Region of interest\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #frame[340: 720,500: 800]\n",
    "                \"\"\"decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(image, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                print(decoded_class_index[0])\n",
    "                if decoded_class_index[0] == 0:\n",
    "                    image = cv2.GaussianBlur(image, (51,51), 50) \"\"\"\n",
    "                    \n",
    "                count+=1\n",
    "                clear_output(wait=True)\n",
    "                imshow(image)\n",
    "                show()\n",
    "                if sequences_number !=-1 :\n",
    "                    if count == sequences_number:\n",
    "                        break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Release the Video Device\n",
    "        cap.release()\n",
    "        # Message to be displayed after releasing the device\n",
    "        print(\"Released Video Resource\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_video(src = \"assets/sex-4.mp4\", count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        imshow(ROI)\n",
    "        show()\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "def parallel_process_video(src = \"assets/sex-4.mp4\",inline = True, figsize = (30, 30), count = 0, limit = 50, hard = True, winStride =(4, 4),padding=(8, 8), scale=1.05):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    # open webcam video stream\n",
    "    \n",
    "    cap = cv2.VideoCapture(src)\n",
    "\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Extract Region of interest\n",
    "        ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        COPY = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "        if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "            if not hard:\n",
    "                (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                # draw the original bounding boxes\n",
    "                for (x, y, w, h) in rects:\n",
    "                    decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                    if decoded_class_index[0]==0:\n",
    "                    #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                        copy = ROI[y:y+h, x:x+w]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                        #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                # apply non-maxima suppression to the bounding boxes using a\n",
    "                # fairly large overlap threshold to try to maintain overlapping\n",
    "                # boxes that are still people\n",
    "                rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                # draw the final bounding boxes\n",
    "                for (xA, yA, xB, yB) in pick:\n",
    "                    copy = ROI[yA:yB, xA:xB]\n",
    "                    blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                    ROI[yA:yB, xA:xB] = blur\n",
    "                    #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "            else:\n",
    "                 ROI = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "        \n",
    "        if inline:\n",
    "            clear_output(wait=True)\n",
    "            \"\"\"plt.subplot(vertical,horizontal,elem_place)\n",
    "            plt.subplots_adjust(hspace = plt_hspace)\n",
    "            plt.title(title)\n",
    "            plt.imshow(image)\"\"\"\n",
    "            plt.figure(figsize=figsize)\n",
    "            subplot(1,2,1)\n",
    "            title(\"neutral\")\n",
    "            imshow(COPY)\n",
    "            subplot(1,2,2)\n",
    "            title(\"processed\")\n",
    "            imshow(ROI)\n",
    "            show()\n",
    "        else:\n",
    "            cv2.imshow(\"neutral\", COPY)\n",
    "            cv2.imshow(\"processed\", ROI)\n",
    "\n",
    "\n",
    "        if limit !=-1 and count == limit:\n",
    "            break\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "def local_video_preprocess(videoPath, hard=True,log=False,saveFrame = True, video_title=\"\", winStride =(4, 4),padding=(8, 8), scale=1.05, overlapThresh=0.65, probs=None, size = (0, 0)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "        \n",
    "        #cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    if not size == (0,0):\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    # Read until video is completed\n",
    "    while(cap.isOpened()):\n",
    "        \n",
    "            \n",
    "      # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        try:\n",
    "                height, width, _ = frame.shape\n",
    "   \n",
    "        except Exception as wrong: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "        # Extract Region of interest\n",
    "        \n",
    "        if ret == True:\n",
    "            ENDROI = frame\n",
    "            ROI = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI, dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "            if decoded_class_index[0]==0:\n",
    "            # resizing for faster detection\n",
    "            # using a greyscale picture, also for faster detection\n",
    "                if not hard:\n",
    "                    (rects, weights) = hog.detectMultiScale(ROI, winStride=winStride, padding=padding, scale=scale)\n",
    "\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        decoded_class_index, decoded_prediction_precision,predictions = decode_prediction(model.predict(np.array([cv2.resize(ROI[y:y+h, x:x+w], dimensions, interpolation = cv2.INTER_AREA)/255])))\n",
    "                        if decoded_class_index[0]==0:\n",
    "                        #blur = cv2.GaussianBlur(ROI, (51,51), 50) \n",
    "                            copy = ROI[y:y+h, x:x+w]\n",
    "                            blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                            ROI[y:y+h, x:x+w] = blur\n",
    "\n",
    "                            #cv2.rectangle(ROI, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    #pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "                    pick = non_max_suppression(rects, probs=probs, overlapThresh=overlapThresh)\n",
    "                    # draw the final bounding boxes\n",
    "                    for (xA, yA, xB, yB) in pick:\n",
    "                        copy = ROI[yA:yB, xA:xB]\n",
    "                        blur = cv2.GaussianBlur(copy, (51,51), 50) \n",
    "                        ENDROI[yA:yB, xA:xB] = blur\n",
    "                        #cv2.rectangle(ROI, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "                else:\n",
    "                     ENDROI = cv2.GaussianBlur(ENDROI, (51,51), 50)\n",
    "            if not size == (0,0):\n",
    "                cv2.resize(ENDROI,size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "            if log:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                bottomLeftCornerOfText = (70*width//100, 95*height//100)#(height-100, width-100)\n",
    "                TopRightCornerOfText = (15*width//100, 15*height//100)\n",
    "                fontScale = 0.8\n",
    "                fontColor = (255, 99, 71) #(255,255,255)\n",
    "                lineType  = 2\n",
    "                cv2.putText(ENDROI,'{0} : {1}'.format(binary_classes_names[int(decoded_class_index)], float(\"{:.2f}\".format(decoded_prediction_precision[0][0]))),  bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n",
    "                if not video_title == \"\":\n",
    "                    cv2.putText(ENDROI,video_title,  TopRightCornerOfText, font, fontScale, fontColor, lineType)\n",
    "            cv2.imshow('Frame',ENDROI)\n",
    "            if saveFrame :\n",
    "                frames.append(ROI)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            \n",
    "\n",
    "          # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1, start=0, end=0):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(figures)\n",
    "    count = 0\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for i in range(start, end):\n",
    "        axeslist.ravel()[i].imshow(figures[i], cmap=plt.jet())\n",
    "        axeslist.ravel()[i].set_title(str(count))\n",
    "        axeslist.ravel()[i].set_axis_off()\n",
    "        count+=1\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos => https://www.youtube.com/c/Wedontwatchtv/videos\n",
    "# current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_video = \"assets/sex-trip-15.mp4\"\n",
    "current_sequences_number = 100\n",
    "limit_sequences_number = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-8efb5322b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallel_process_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_video\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_sequences_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_sequences_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-8d65b8d6993c>\u001b[0m in \u001b[0;36mparallel_process_video\u001b[0;34m(src, inline, figsize, count, limit, hard, winStride, padding, scale)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mCOPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_prediction_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoded_class_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# resizing for faster detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-ec2fc7a40586>\u001b[0m in \u001b[0;36mdecode_prediction\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnumpy_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbinary_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "parallel_process_video(current_video,count=current_sequences_number, limit=limit_sequences_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = {\n",
    "    \"sex-trip\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 35,\n",
    "        \"base_name\": \"sex-trip-\"\n",
    "    },\n",
    "    \"porn\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 3,\n",
    "        \"base_name\": \"porn-\"\n",
    "    },\n",
    "    \"sex\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 5,\n",
    "        \"base_name\": \"sex-\"\n",
    "    },\n",
    "    \"normal\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 7,\n",
    "        \"base_name\": \"normal-\"\n",
    "    },\n",
    "    \"normal-sexy\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 10,\n",
    "        \"base_name\": \"normal-sexy-\"\n",
    "    },\n",
    "    \"sexy-woman\":{\n",
    "        \"local_prep_start\": 1,\n",
    "        \"local_prep_end\": 13,\n",
    "        \"base_name\": \"sexy-woman-\"\n",
    "    }\n",
    "}\n",
    "\n",
    "key = \"sexy-woman\" #porn, sex, sex-trip,sexy-woman, normal\n",
    "\n",
    "base_name = prepared_data[key][\"base_name\"]\n",
    "\n",
    "local_prep_start = prepared_data[key][\"local_prep_start\"]\n",
    "local_prep_end = prepared_data[key][\"local_prep_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(local_prep_start, local_prep_end):\n",
    "    try:\n",
    "        local_video_preprocess(\"assets/{0}{1}.mp4\".format(base_name, i),log=True,video_title = \"{0}{1}\".format(base_name, i), hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "    except Exception as wrong: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = local_video_preprocess(\"assets/sex-1.mp4\",log=True, hard=True, winStride =(4, 4),padding=(20, 20), scale=1.2, overlapThresh=0.25, probs=None, size=(100, 100))\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(frames, 3, 4, end=12)\n",
    "plt.figsize=(50, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames,path=\"images_saves/adult\", start=0, end=0, tread=1, random=False, image_number=0):\n",
    "    if random:\n",
    "        if image_number == 0:\n",
    "            image_number = len(frames)-1\n",
    "            \n",
    "        generated = []\n",
    "        for i in range(0, image_number):\n",
    "            current_id = randint(0, len(frames))\n",
    "            while current_id in generated:\n",
    "                current_id = randint(0, len(frames))\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[current_id], cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "    else:  \n",
    "        if end == 0:\n",
    "            end = len(frames)\n",
    "        count=0\n",
    "        while (end - start - count) > 0:\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            count+=tread\n",
    "\n",
    "        \"\"\"for i in range(start, end):\n",
    "            filename = path+\"/\"+str(uuid.uuid1())+\".jpg\"\n",
    "            cv2.imwrite(filename, cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB))\n",
    "            if tread>1:\n",
    "                i+=(tread-1)\"\"\"\n",
    "        \n",
    "def randomize_frames(frames, image_number=0):\n",
    "    output_frames = []\n",
    "    if image_number == 0:\n",
    "        image_number = len(frames)-1  \n",
    "    generated = []\n",
    "    for i in range(0, image_number):\n",
    "        current_id = randint(0, len(frames))\n",
    "        while current_id in generated:\n",
    "            current_id = randint(0, len(frames))\n",
    "        output_frames.append(frames[current_id])\n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, tread=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames(frames, random=True,image_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_frames = []\n",
    "for frame in randomize_frames(frames, 40):\n",
    "    batch_frames.append(cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch_frames = model.predict(numpy.array(batch_frames))\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch_frames = tf.squeeze(predicted_batch_frames)#.numpy()\n",
    "\n",
    "predicted_ids , precisions, preds = decode_prediction(predicted_batch_frames)\n",
    "\n",
    "predicted_class_names = []\n",
    "for i in predicted_ids:\n",
    "    predicted_class_names.append(class_names[i])\n",
    "    \n",
    "print(\"Labels:           \", predicted_class_names)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "print(\"precisions : \", precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "#detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" #if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    #detector_images.append(batch_frames[i])\n",
    "    plt.imshow(batch_frames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.8969703  -10.857968    -3.1800833   -3.9249196    0.27488637\n",
      "   -2.2344272 ]\n",
      " [ -1.2776935   -6.3090925   -6.899217    -1.1201884    0.12650278\n",
      "   -2.5715902 ]\n",
      " [ -5.3111796    8.472974     0.8620315   -8.575378    -9.987681\n",
      "   -4.2203956 ]\n",
      " [ -2.1316488    8.168974    -2.078558    -5.6624618   -5.399054\n",
      "   -5.771137  ]\n",
      " [ -4.663002     5.7697      -0.5324979   -6.004955    -4.259072\n",
      "   -4.344286  ]\n",
      " [ -1.109822    -5.878892    -6.6231585   -3.1851692   -1.1893387\n",
      "   -3.4334447 ]\n",
      " [ -5.18945     -5.000453     4.6586523   -6.9442935   -8.525067\n",
      "  -13.542187  ]\n",
      " [ -0.88114905   1.9464777   -5.7865834   -4.092399    -3.2847898\n",
      "   -5.46647   ]\n",
      " [ -6.449512    -2.925442     2.7264435   -7.8307095   -3.9950268\n",
      "   -9.709136  ]\n",
      " [ -0.34056193  -6.7364106   -4.601235    -0.49990138  -0.4304405\n",
      "   -1.5408584 ]\n",
      " [ -4.3549933   -6.034152     0.692939    -5.4412785   -4.8391623\n",
      "   -9.99218   ]\n",
      " [ -2.6016297   -5.525449     3.829113    -5.2505198   -9.194032\n",
      "  -13.535313  ]\n",
      " [ -2.084867    -9.693953    -4.2929063    3.1746817   -6.410122\n",
      "   -7.720274  ]\n",
      " [ -1.2036457   -7.173782     0.71969926  -2.731968   -11.5220175\n",
      "  -13.906232  ]\n",
      " [ -2.445477    -5.701789    -4.808822    -2.3955004    3.716369\n",
      "    0.5146834 ]\n",
      " [  1.9596744   -6.992357    -4.7286696   -2.206419    -3.1405451\n",
      "   -6.0356917 ]\n",
      " [ -4.6970778   -9.37735      2.0965018   -4.9033856   -5.6815705\n",
      "   -8.3706665 ]\n",
      " [ -3.7258627   -5.2141523    3.3352299   -5.672403    -4.760076\n",
      "  -10.612717  ]\n",
      " [ -1.4555168   -8.995       -6.1780324   -0.50453603  -0.65847206\n",
      "   -1.8677595 ]\n",
      " [ -1.0808804   -9.934258    -5.52584     -1.5892256    0.48933256\n",
      "   -5.178729  ]\n",
      " [  1.4614711   -8.311919    -4.7199106    1.4778228   -6.8310966\n",
      "  -14.38678   ]\n",
      " [ -2.4587302   -6.9902663    4.875345    -5.0933933   -9.415818\n",
      "   -9.7316    ]\n",
      " [  0.80652285 -11.194153    -5.5146112    3.147697   -13.524586\n",
      "  -13.13147   ]\n",
      " [ -1.521724    -5.200651     1.3619093   -4.008108    -8.764813\n",
      "  -12.08158   ]\n",
      " [ -4.8046384    7.7190585    0.60237837  -6.056597    -9.690414\n",
      "   -7.1947246 ]\n",
      " [ -0.06324947 -10.190221    -8.617987     1.6794635   -8.145882\n",
      "  -11.252631  ]\n",
      " [  2.4248571   -9.340138    -6.2754164    1.000825    -9.082387\n",
      "  -12.116064  ]\n",
      " [ -2.114965    -4.3396916   -4.4192452   -4.2271757   -2.6898825\n",
      "   -5.1975503 ]\n",
      " [ -3.0891793    7.0240426   -3.335868    -4.8234243   -5.3107476\n",
      "   -3.9639492 ]\n",
      " [ -2.6315043   -9.096172     4.150396    -4.5658555   -7.5611243\n",
      "   -9.766409  ]\n",
      " [  2.8115427  -11.046545    -4.8750215    0.5790534  -14.578028\n",
      "  -12.734627  ]\n",
      " [ -4.150209    -0.0978792   -5.453952    -5.188823    -2.1214685\n",
      "   -5.648204  ]], shape=(32, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(validation_set))\n",
    "label_batch = label_batch.astype(int)\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "#interpretation_batch = tf.keras.applications.mobilenet.decode_predictions(predicted_batch)\n",
    "#print(interpretation_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch)#.numpy()\n",
    "decoded_class_index = []\n",
    "decode_prediction_precision = []\n",
    "\n",
    "for prediction in predicted_batch:\n",
    "    result = 0 if prediction < 0.5 else 1\n",
    "    precision = calculate_average(prediction)\n",
    "    decoded_class_index.append(result)\n",
    "    decode_prediction_precision.append(precision)\n",
    "    print(np.array(decoded_class_index), np.array(decode_prediction_precision),predictions)\n",
    "\n",
    "\n",
    "\n",
    "# predicted_ids , precisions, preds = decode_prediction(predicted_batch)\n",
    "\n",
    "# predicted_class_names = []\n",
    "# for i in predicted_ids:\n",
    "#     predicted_class_names.append(class_names[i])\n",
    "    \n",
    "# print(\"Labels:           \", label_batch)\n",
    "# print(\"Predicted labels: \", predicted_ids)\n",
    "# print(\"precisions : \", precisions)\n",
    "\n",
    "# cfs_matrix = tf.math.confusion_matrix(\n",
    "#     label_batch, predicted_ids, num_classes=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preview predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import ndimage\n",
    "%matplotlib inline \n",
    "rangeTot = 30\n",
    "rangeStart = 20\n",
    "\n",
    "rangeDiff = rangeTot - rangeStart\n",
    "figsize = (40, 40)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "detector_images = []\n",
    "for i in range(rangeStart, rangeTot):\n",
    "    plt.subplot(rangeDiff,int((rangeDiff)/2),i+1)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    color = \"blue\" if predicted_ids[i] == label_batch[i] else \"red\"\n",
    "    plt.title(str(float(\"{:.2f}\".format(precisions[i])))+\" -> \"+predicted_class_names[i]+\" pred : \"+str(float(\"{:.2f}\".format(preds[i]))), color=color)\n",
    "    #plt.imshow(image_batch[i]/255 if label_batch[i]==0 else ndimage.gaussian_filter(image_batch[i]/255, sigma=10))\n",
    "    detector_images.append(image_batch[i])\n",
    "    plt.imshow(image_batch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdni.pornpics.com/460/1/44/70070362/70070362_008_1429.jpg\"\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "image = np.asarray(bytearray(req.content), dtype=\"uint8\")\n",
    "imageBGR = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "imageRGB = cv2.cvtColor(imageBGR , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = detect_adult_picture_no_plot(imageRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1040, 1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-adult.txt\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://source.unsplash.com/random\", \n",
    "    \"https://source.unsplash.com/random\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    #detect_adult_picture(image, prod=True, pass_neutral=True, figsize=(30, 30), WIDTH = 600, PYR_SCALE = 1.5, WIN_STEP = 16, ROI_SIZE = (250, 250), INPUT_SIZE = (224, 224))\n",
    "    detect_adult_picture_from_url(url, True, False, probaLimit = 0.1, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 12, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 23, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-test.txt\", 32,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in range(0, 10):\n",
    "    urls.append(\"https://source.unsplash.com/random\")\n",
    "    \n",
    "predict_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_txt_urls(\"deploy-neutral.txt\", 1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://data.whicdn.com/images/309065672/superthumb.jpg?t=1521271196\",\n",
    "    \"https://data.whicdn.com/images/299468608/superthumb.jpg?t=1508189155\",\n",
    "    \"https://data.whicdn.com/images/298428675/superthumb.jpg?t=1506897335\",\n",
    "    \"https://data.whicdn.com/images/296803163/superthumb.jpg?t=1505000487\",\n",
    "    \"https://data.whicdn.com/images/295035854/superthumb.jpg?t=1503153983\",\n",
    "    \"https://data.whicdn.com/images/294438077/superthumb.jpg?t=1502537206\",\n",
    "    \"https://data.whicdn.com/images/294393942/superthumb.jpg?t=1502484576\",\n",
    "    \"https://data.whicdn.com/images/294393884/superthumb.jpg?t=1502484540\",\n",
    "    \"https://data.whicdn.com/images/294393780/superthumb.jpg?t=1502484473\"\n",
    "]        \n",
    "predict_from_urls(urls)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccde67e4faa8fac03f67c61d4d2d25acf63db2b953068fc2e967f42f8fdbc53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('holypics-SxDLhKSZ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
